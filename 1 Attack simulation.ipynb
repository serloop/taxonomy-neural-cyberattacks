{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from brian2 import *\n",
    "import csv\n",
    "import random\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_range_V = 5\n",
    "max_range_V = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_attack_generation = [\"FLO\", \"JAM\", \"SCA\", \"FOR\", \"SPO\", \"SYB\", \"SIN\", \"NON\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_dir = \"results/\"\n",
    "plots_dir = \"plots/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods for csv export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_csv_file(filename, line):\n",
    "    # Update the attack file\n",
    "    with open(filename, 'a') as csvFile:\n",
    "        writer = csv.writer(csvFile, delimiter=';')\n",
    "        writer.writerow(line)\n",
    "\n",
    "def dump_simulation_data_to_csv(attack, test, position_attack, n_attacks, n_neurons, attacked_neurons, coord_attack, stim_value, n_exec, vIncrement, paramI, monitor, filename):\n",
    "    mon_trains = monitor.spike_trains()\n",
    "    \n",
    "    for neuron in mon_trains.keys():\n",
    "        for time_delta in mon_trains[neuron]:\n",
    "            append_to_csv_file(filename, [attack, test, position_attack, n_attacks, n_neurons, attacked_neurons, coord_attack, stim_value, n_exec, str(vIncrement), str(paramI), round(time_delta/ms, 1), neuron])\n",
    "\n",
    "def list_neurons_to_string(list_neurons):\n",
    "    result = \"\"\n",
    "    for neuron in list_neurons:\n",
    "        result += str(neuron)+\"-\"\n",
    "        \n",
    "    return result[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General variables and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_coordinates_optimal_path = [[0,0], [1,0], [1,1], [1,2], [0,2], [0,3], [0,4], [0,5], [1,5], [2,5], \n",
    "                                 [2,4], [2,3], [3,3], [3,2], [3,1], [3,0], [4,0], [5,0], [6,0], [6,1], \n",
    "                                 [6,2], [5,2], [5,3], [5,4], [5,5], [6,5], [6,6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_TIME = 1000*ms\n",
    "\n",
    "# STEP_TIME per movement. As there are 27 positions -> 27*STEP_TIME. \n",
    "SIMULATION_TIME = (STEP_TIME/ms*len(list_coordinates_optimal_path))*ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMULATION_TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duration of simulation, after STEP_TIME. Divided by the number of neurons\n",
    "def get_time_steps_sequential(tAttack):\n",
    "    return trunc(((SIMULATION_TIME-tAttack)/200)/ms)*ms\n",
    "\n",
    "def get_number_attacks_per_position_sequential(tAttack):\n",
    "    return STEP_TIME/get_time_steps_sequential(tAttack)\n",
    "\n",
    "def get_last_instant_attack_sequential(tAttack):\n",
    "    return tAttack+get_time_steps_sequential(tAttack)*200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_time_steps_sequential(50*ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_number_attacks_per_position_sequential(50*ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_last_instant_attack_sequential(50*ms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary methods to translate between coordinates and indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_list_random_neurons(nNeurons):\n",
    "    list_neurons = list(range(0, 200))\n",
    "    result = []\n",
    "\n",
    "    for i in range(0, nNeurons):\n",
    "        index = randint(0, len(list_neurons))\n",
    "        \n",
    "        result.append(list_neurons[index])\n",
    "        del list_neurons[index]\n",
    "    \n",
    "    return sorted(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict that stores the index value of a neuron: 3D coordinate -> numeric index (0-199)\n",
    "dict_neurons_to_numbers = {}\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for i in range(0, 5):\n",
    "    for j in range(0, 5): \n",
    "        for k in range(0, 8): \n",
    "            dict_neurons_to_numbers[(i,j,k)] = counter\n",
    "            counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_neurons_to_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict that stores the instant in which the mouse is in eah position of the optimal path\n",
    "\n",
    "dict_instant_optimal_path = {}\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for coord in list_coordinates_optimal_path:\n",
    "    dict_instant_optimal_path[(coord[0],coord[1])]= counter\n",
    "    counter += STEP_TIME/ms     \n",
    "\n",
    "# List of the instants previously calculated (list values of dict)\n",
    "list_values = list(dict_instant_optimal_path.values())\n",
    "list_instant_optimal_path = []\n",
    "\n",
    "for value in list_values:\n",
    "    list_instant_optimal_path.append(round(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_visible_coordinates_by_position(x,y):\n",
    "    \n",
    "    coords = [[x-1,y-1], [x-1, y], [x-1,y+1], [x,y-1], [x,y], [x,y+1], [x+1,y-1], [x+1,y], [x+1,y+1]]\n",
    "    result = []\n",
    "    \n",
    "    for n in coords:\n",
    "        if (0 <= n[0] <= 6) and (0 <= n[1] <= 6):\n",
    "            result.append(n)\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_visible_coordinates_by_position(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the neurons indexes that are related to a given maze coordinate\n",
    "def get_neurons_indexes_by_position(x, y):\n",
    "    \n",
    "    coords = [\n",
    "        [x-2,y-2,0], [x-2,y-2,1], [x-2,y-2,2], [x-2,y-2,3], [x-2,y-2,4], [x-2,y-2,5], [x-2,y-2,6], [x-2,y-2,7],\n",
    "        [x-2,y-1,0], [x-2,y-1,1], [x-2,y-1,2], [x-2,y-1,3], [x-2,y-1,4], [x-2,y-1,5], [x-2,y-1,6], [x-2,y-1,7],\n",
    "        [x-2,y,0], [x-2,y,1], [x-2,y,2], [x-2,y,3], [x-2,y,4], [x-2,y,5], [x-2,y,6], [x-2,y,7],\n",
    "        [x-1,y-2,0], [x-1,y-2,1], [x-1,y-2,2], [x-1,y-2,3], [x-1,y-2,4], [x-1,y-2,5], [x-1,y-2,6], [x-1,y-2,7],\n",
    "        [x-1,y-1,0], [x-1,y-1,1], [x-1,y-1,2], [x-1,y-1,3], [x-1,y-1,4], [x-1,y-1,5], [x-1,y-1,6], [x-1,y-1,7],\n",
    "        [x-1,y,0], [x-1,y,1], [x-1,y,2], [x-1,y,3], [x-1,y,4], [x-1,y,5], [x-1,y,6], [x-1,y,7],\n",
    "        [x,y-2,0], [x,y-2,1], [x,y-2,2], [x,y-2,3], [x,y-2,4], [x,y-2,5], [x,y-2,6], [x,y-2,7],\n",
    "        [x,y-1,0], [x,y-1,1], [x,y-1,2], [x,y-1,3], [x,y-1,4], [x,y-1,5], [x,y-1,6], [x,y-1,7],\n",
    "        [x,y,0], [x,y,1], [x,y,2], [x,y,3], [x,y,4], [x,y,5], [x,y,6], [x,y,7],\n",
    "    ]\n",
    "    \n",
    "    coord_result = []\n",
    "    result = []\n",
    "    \n",
    "    for n in coords:\n",
    "        if (0 <= n[0] <= 4) and (0 <= n[1] <= 4):\n",
    "            coord_result.append(n)\n",
    "            \n",
    "    for coord in coord_result:\n",
    "        result.append(dict_neurons_to_numbers[(coord[0],coord[1],coord[2])])\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get_neurons_indexes_by_position(0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_neurons_visible_positions(x, y):\n",
    "    # Get the positions visible from the current position\n",
    "    visible_coords = get_visible_coordinates_by_position(x, y)\n",
    "    \n",
    "    # Get the complete list of neurons related to all visible positions\n",
    "    list_neurons_index = []\n",
    "\n",
    "    for coord in visible_coords:\n",
    "        neurons = get_neurons_indexes_by_position(coord[0], coord[1])\n",
    "        \n",
    "        for n in neurons:\n",
    "            list_neurons_index.append(n)\n",
    "    \n",
    "    # Remove duplicates in list_neurons_index\n",
    "    result = []\n",
    "    \n",
    "    for n in list_neurons_index:\n",
    "        if n not in result:\n",
    "            result.append(n)\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get_related_neurons_visible_positions(0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all neurons related to all positions, in a dict. Both list of neurons and count\n",
    "dict_all_neurons = {}\n",
    "dict_all_neurons_count = {}\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for pos in list_coordinates_optimal_path:\n",
    "    dict_all_neurons[counter] = get_related_neurons_visible_positions(pos[0], pos[1])\n",
    "    dict_all_neurons_count[counter] = len(get_related_neurons_visible_positions(pos[0], pos[1]))\n",
    "    \n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary methods for Brian Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print plot for a state monitor\n",
    "def plot_state_monitor(monitor, neuron):\n",
    "    plot(monitor.t/ms, monitor.v[neuron]/mV)\n",
    "    xlabel('Time (ms)')\n",
    "    ylabel('v');\n",
    "    \n",
    "sns.set(style=\"whitegrid\",font_scale=2.5, rc={'figure.figsize':(60,20)})\n",
    "\n",
    "\n",
    "def seaborn_state_monitor(monitor, neuron): \n",
    "    ax = sns.lineplot(monitor.t/ms, monitor.v[neuron]/mV, linewidth=3.0)\n",
    "    \n",
    "    '''\n",
    "    cont = STEP_TIME/ms\n",
    "    for a in range(0, len(list_coordinates_optimal_path)+1):\n",
    "        plt.axvline(x=cont*a, color=\"red\")\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(25))\n",
    "    '''\n",
    "    plt.plot()\n",
    "    \n",
    "def seaborn_state_monitor_range(monitor, neuron, init_time, end_time):    \n",
    "    ax = sns.lineplot(monitor.t[init_time*10:end_time*10]/ms, monitor.v[neuron][init_time*10:end_time*10]/mV, linewidth=4.0)\n",
    "    \n",
    "    ax.set_xlabel(\"Time\",fontsize=50, fontweight='bold')\n",
    "    \n",
    "    ax.tick_params(labelsize=42)\n",
    "    ax.tick_params(labelsize=42)\n",
    "    \n",
    "    plt.plot()\n",
    "    \n",
    "def seaborn_state_monitor_range_comparison(monitor1, monitor2, neuron, init_time, end_time):    \n",
    "    fig,axs = plt.subplots(nrows=2, ncols=1, sharey='row', sharex='col', figsize=(80,40))\n",
    "    fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.01, hspace=0.07)\n",
    "    \n",
    "    sns.lineplot(monitor1.t[init_time*10:end_time*10]/ms, monitor1.v[neuron][init_time*10:end_time*10]/mV, linewidth=4.0, ax=axs[0])\n",
    "    sns.lineplot(monitor2.t[init_time*10:end_time*10]/ms, monitor2.v[neuron][init_time*10:end_time*10]/mV, linewidth=4.0, ax=axs[1])\n",
    "    \n",
    "    axs[1].set_xlabel(\"Time\",fontsize=50, fontweight='bold')\n",
    "    \n",
    "    axs[0].tick_params(labelsize=42)\n",
    "    axs[0].set_title(\"Spontaneous\", size=50)\n",
    "    \n",
    "    axs[1].tick_params(labelsize=42)\n",
    "    axs[1].set_title(\"Attack\", size=50)\n",
    "    \n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTIONS TO LOAD DATA FROM EXTERNAL FILES (TOPOLOGY, WEIGHTS, PARAMETERS...) ###\n",
    "\n",
    "def getSynapsisDataFromFile(filename):\n",
    "    synapsysData = []\n",
    "    minWeight = 0\n",
    "    maxWeight = 0\n",
    "    \n",
    "    with open(filename) as csvfile:\n",
    "        csvReader = csv.reader(csvfile, delimiter=';')\n",
    "        next(csvReader, None) # skip header\n",
    "        \n",
    "        firstRow =  next(csvReader)\n",
    "        synapsysData.append([int(firstRow[0]), int(firstRow[1]), float(firstRow[2])])\n",
    "        minWeight = float(firstRow[2])\n",
    "        maxWeight = float(firstRow[2])\n",
    "        \n",
    "        for row in csvReader:\n",
    "            synapsysData.append([int(row[0]), int(row[1]), float(row[2])])\n",
    "            \n",
    "            if minWeight > float(row[2]):\n",
    "                minWeight = float(row[2])\n",
    "            if maxWeight < float(row[2]):\n",
    "                maxWeight = float(row[2])\n",
    "    \n",
    "    return synapsysData, minWeight, maxWeight\n",
    "\n",
    "def loadIzhikevichParamI(fileName):\n",
    "    with open(fileName, 'r') as reader:\n",
    "        I = [line.rstrip('\\n') for line in reader]\n",
    "        \n",
    "        for i in range(0, len(I)):\n",
    "            I[i] = float(I[i])*mV/ms\n",
    "        \n",
    "        return I\n",
    "    \n",
    "def load_initial_voltages(fileName):\n",
    "    with open(fileName, 'r') as reader:\n",
    "        v = [line.rstrip('\\n') for line in reader]\n",
    "        \n",
    "        for i in range(0, len(v)):\n",
    "            v[i] = float(v[i])*mV\n",
    "        \n",
    "        return v\n",
    "\n",
    "\n",
    "def exportDataStateMonitor(dataStateMon):\n",
    "    dataStoreStateMon = []\n",
    "    \n",
    "    # STATEMON\n",
    "    # attr (v), time (0-999), neuron (0-199)\n",
    "    # if execTime == 100ms, len is 1k iterations\n",
    "    for time in range(0, len(dataStateMon['v'])):\n",
    "        timeData = []\n",
    "\n",
    "        for neuron in range(0, len(dataStateMon['v'][time])):\n",
    "            timeData.append(dataStateMon['v'][time][neuron])\n",
    "        \n",
    "        dataStoreStateMon.append(timeData)\n",
    "\n",
    "     # Result: for each instant, the V of each single neuron\n",
    "    \n",
    "    return dataStoreStateMon\n",
    "\n",
    "\n",
    "def exportDataSpikeMonitor(dataSpikeMon):          \n",
    "    # SPIKEMON\n",
    "    # \"t\": time of each spike during the simulation\n",
    "    # \"i\": nb of neuron that spikes, in chronological order. \n",
    "    # \"count\": indicates, for each neuron, the nb of spikes during the simulation\n",
    "\n",
    "    # Result: array of 2 positions that contains:\n",
    "    #     0) array where each position has the binome <time instant, nb of neuron>\n",
    "    #     1) the array \"count\" with the nb of spikes per neuron\n",
    "    #\n",
    "    # Precondition: nb elements in \"t\" == nb elments in \"i\"\n",
    "    \n",
    "    dataStoreSpikeMon = []\n",
    "    timeSpikeMon = []\n",
    "    countSpikeMon = []\n",
    "\n",
    "    for time in range(0, len(dataSpikeMon['t'])):\n",
    "        # Each position: <time, nNeuron>\n",
    "        timeSpikeMon.append([dataSpikeMon['t'][time], dataSpikeMon['i'][time]])\n",
    "    \n",
    "    for nSpikes in range(0, len(dataSpikeMon['count'])):\n",
    "        # Each position: nSpikes\n",
    "        countSpikeMon.append(dataSpikeMon['count'][nSpikes])\n",
    "    \n",
    "    dataStoreSpikeMon = [timeSpikeMon, countSpikeMon]\n",
    "    \n",
    "    return dataStoreSpikeMon\n",
    "\n",
    "\n",
    "def exportAllDataMonitors(stateMons, spikeMons):\n",
    "    # Generate data to export to CSV (suitable format for later processing)\n",
    "    dataStateMon1 = exportDataStateMonitor(stateMons[0].get_states(units=False))\n",
    "    dataStateMon2 = exportDataStateMonitor(stateMons[1].get_states(units=False))\n",
    "    dataStateMon3 = exportDataStateMonitor(stateMons[2].get_states(units=False))\n",
    "\n",
    "    dataSpikeMon1 = exportDataSpikeMonitor(spikeMons[0].get_states(units=False))\n",
    "    dataSpikeMon2 = exportDataSpikeMonitor(spikeMons[1].get_states(units=False))\n",
    "    dataSpikeMon3 = exportDataSpikeMonitor(spikeMons[2].get_states(units=False))\n",
    "    \n",
    "    return [dataStateMon1, dataSpikeMon1, dataStateMon2, dataSpikeMon2, dataStateMon3, dataSpikeMon3]\n",
    "\n",
    "### FUNCTIONS TO GENERATE RANDOM NUMBER OF NEURONS ###\n",
    "\n",
    "def generateRandomNeurons(currentNeurons):\n",
    "    selectedNeurons = []\n",
    "    \n",
    "    # Generate random coordinates for the current number of neurons\n",
    "    for currentRandom in range(0, currentNeurons):\n",
    "\n",
    "        randNeuron = np.random.randint(0, 200)\n",
    "\n",
    "        # Avoid duplicates\n",
    "        while (randNeuron in selectedNeurons):\n",
    "            randNeuron = np.random.randint(0, 200)\n",
    "\n",
    "        # Store the selected neuron\n",
    "        selectedNeurons.append(randNeuron)\n",
    "        \n",
    "    return selectedNeurons\n",
    "\n",
    "    \n",
    "def generatePairsRandomNeurons(currentNeurons):    \n",
    "    selectedNeurons = []\n",
    "    selectedPairsNeurons = []\n",
    "    \n",
    "    # Generate random coordinates for the current number of neurons -> pairs of neurons\n",
    "    for currentRandom in range(0, currentNeurons):\n",
    "        # Neuron 1 of the pair\n",
    "        randNeuron1 = np.random.randint(0, 200)\n",
    "\n",
    "        # Avoid duplicates for neuron 1\n",
    "        while (randNeuron1 in selectedNeurons):\n",
    "            randNeuron1 = np.random.randint(0, 200)\n",
    "\n",
    "        selectedNeurons.append(randNeuron1)\n",
    "\n",
    "        # Neuron 2 of the pair\n",
    "        randNeuron2 = np.random.randint(0, 200)\n",
    "\n",
    "        # Avoid duplicates for neuron 2\n",
    "        while (randNeuron2 in selectedNeurons):\n",
    "            randNeuron2 = np.random.randint(0, 200)\n",
    "\n",
    "        selectedNeurons.append(randNeuron2)\n",
    "\n",
    "        selectedPairsNeurons.append([randNeuron1, randNeuron2])\n",
    "        \n",
    "    return selectedPairsNeurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load simulation parameters from files (weights, synapsis, I param...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### VARIABLES FOR THE SIMULATION OF THE ATTACKS ###\n",
    "\n",
    "BASIC_MODEL = 0\n",
    "IZHIKEVICH_MODEL = 1\n",
    "\n",
    "# Load only once synapsis and weights\n",
    "#dataSynapsisMaze_Conv1, minWeightsMaze_Conv1, maxWeightsMaze_Conv1 = getSynapsisDataFromFile(\"synapsysMaze-Conv1.csv\")\n",
    "dataSynapsisConv1_Conv2, minWeightsConv1_Conv2, maxWeightsConv1_Conv2 = getSynapsisDataFromFile(\"synapsysConv1-Conv2.csv\")\n",
    "dataSynapsisConv2_Dense, minWeightsConv2_Dense, maxWeightsConv2_Dense = getSynapsisDataFromFile(\"synapsysConv2-Dense.csv\")\n",
    "\n",
    "# Process the data for the simulator\n",
    "initSourceNeuronsConv1_Conv2 = []\n",
    "initTargetNeuronsConv1_Conv2 = []\n",
    "initSourceNeuronsConv2_Dense = []\n",
    "initTargetNeuronsConv2_Dense = []\n",
    "initWeightsMaze_Conv1 = []\n",
    "initWeightsConv1_Conv2 = []\n",
    "initWeightsConv2_Dense = []\n",
    "\n",
    "for syn in range(0, len(dataSynapsisConv1_Conv2)):\n",
    "    initSourceNeuronsConv1_Conv2.append(dataSynapsisConv1_Conv2[syn][0])\n",
    "    initTargetNeuronsConv1_Conv2.append(dataSynapsisConv1_Conv2[syn][1])\n",
    "    initWeightsConv1_Conv2.append(dataSynapsisConv1_Conv2[syn][2])\n",
    "\n",
    "for syn in range(0, len(dataSynapsisConv2_Dense)):\n",
    "    initSourceNeuronsConv2_Dense.append(dataSynapsisConv2_Dense[syn][0])\n",
    "    initTargetNeuronsConv2_Dense.append(dataSynapsisConv2_Dense[syn][1])\n",
    "    initWeightsConv2_Dense.append(dataSynapsisConv2_Dense[syn][2])\n",
    "\n",
    "\n",
    "# Normalize weights for the Izhikevich model\n",
    "np_initWeightsConv1_Conv2 = np.array(initWeightsConv1_Conv2)\n",
    "np_initWeightsConv2_Dense = np.array(initWeightsConv2_Dense)\n",
    "\n",
    "norm_initWeightsConv1_Conv2 = np.interp(np_initWeightsConv1_Conv2, (np_initWeightsConv1_Conv2.min(), np_initWeightsConv1_Conv2.max()), (min_range_V, max_range_V))\n",
    "norm_initWeightsConv2_Dense = np.interp(np_initWeightsConv2_Dense, (np_initWeightsConv2_Dense.min(), np_initWeightsConv2_Dense.max()), (min_range_V, max_range_V))\n",
    "\n",
    "N_NEURONS_MAX = 100\n",
    "N_NEURONS_MIN = 1\n",
    "\n",
    "attacks_dict = {\n",
    "    'FLO': ['Flooding', N_NEURONS_MAX, [0.25, 0.5, 0.75, 1.0]], # Stim multiple neurons per t.u.\n",
    "    'JAM': ['Jamming', N_NEURONS_MAX, [-1.0]], # Inhibit multiple neurons per t.u.\n",
    "    'SCA': ['PortScanning', N_NEURONS_MIN, [0.25, 0.5, 0.75, 1.0]],  # Stim 1 neuron per t.u.\n",
    "    'FOR': ['SelectiveForwarding', N_NEURONS_MIN, [-1.0]], # Inhibit 1 neuron per t.u.\n",
    "    'SPO': ['Spoofing', int(N_NEURONS_MAX/2)], # the attack selects pairs of neurons -> double of this number\n",
    "    'SYB': ['Sybil', N_NEURONS_MAX],\n",
    "}\n",
    "\n",
    "maze =  np.array([\n",
    "   [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  0.,  1.,  0.], \n",
    "    [ 0.,  0.,  0.,  1.,  1.,  1.,  0.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  0.,  1.],\n",
    "    [ 1.,  0.,  0.,  0.,  1.,  1.,  1.],\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.]\n",
    "])\n",
    "\n",
    "# Load initial voltages for all simulation (previously generated random values)\n",
    "v_initial = load_initial_voltages(\"initial_voltage.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_result_simulation(dict_result, neuron, instant):\n",
    "    if neuron not in dict_result.keys():\n",
    "        dict_result[neuron] = []\n",
    "\n",
    "    dict_result[neuron].append(instant)\n",
    "    \n",
    "def neuronal_simulation(simulation_duration, params, results):\n",
    "    #isSequentialAttack, timeStepsSeqAttacks, instantsAttack, stimValue, simulationDuration, neuronList, vIncrement, paramI\n",
    "    \n",
    "    start_scope()\n",
    "    \n",
    "    #defaultclock.dt = 0.1*ms\n",
    "    \n",
    "    # Initial parameters for the equations (typical values)\n",
    "    # a = 0.02/ms\n",
    "    # b = 0.2/ms\n",
    "    # c = -65*mV\n",
    "    # d = 2*mV/ms\n",
    "    # I = 0*mV/ms\n",
    "\n",
    "    # Equations of the Izhikevich neuron model\n",
    "    eqs = '''\n",
    "    dv/dt = (0.04/ms/mV)*v**2+(5/ms)*v+140*mV/ms-u + I : volt\n",
    "    du/dt = a*(b*v-u) : volt/second\n",
    "    I : volt/second\n",
    "    a : Hz\n",
    "    b : Hz\n",
    "    c : volt\n",
    "    d : volt/second\n",
    "    neuronCounter : 1\n",
    "    positionCounter : 1\n",
    "    isFirstTime : 1\n",
    "    timeCounter : second\n",
    "    '''\n",
    "\n",
    "    # Reset of Izhikevich model\n",
    "    reset ='''\n",
    "    v = c\n",
    "    u += d\n",
    "    '''\n",
    "\n",
    "    thresholdValue = 30\n",
    "    resetValue = -65\n",
    "\n",
    "    # Definition of the 1st layer of the CNN\n",
    "    G = NeuronGroup(276, eqs, threshold='v >= thresholdValue*mV', reset=reset, method='euler')\n",
    "    # Initialise variables of all neurons (typical values)\n",
    "\n",
    "    #G.v = resetValue*mV\n",
    "    G.v = v_initial\n",
    "    \n",
    "    G.u = -13*mV/ms # b*v -> 0.2*-65 = -13\n",
    "    G.a = 0.02/ms\n",
    "    G.b = 0.2/ms\n",
    "    G.c = resetValue*mV\n",
    "    G.d = 8*mV/ms\n",
    "    #G.I = paramI*mV/ms\n",
    "    #G.I = loadIzhikevichParamI(\"paramI.txt\")\n",
    "    G.I = 10*mV/ms\n",
    "    \n",
    "    # Create subgroups\n",
    "    layerConv1 = G[:200]\n",
    "    layerConv2 = G[200:272]\n",
    "    layerDense = G[272:276]\n",
    "    \n",
    "    # We only store the counters for the first neuron of layer1 (more efficient)   \n",
    "    layerConv1.neuronCounter[0] = 0\n",
    "    layerConv1.positionCounter[0] = 0\n",
    "    layerConv1.timeCounter[0] = 0*ms\n",
    "    layerConv1.isFirstTime[0] = 1\n",
    "    \n",
    "    # SPO uses timeCounter as a list index counter\n",
    "    if(params[\"attack\"] == \"SPO\"):\n",
    "        layerConv1.timeCounter[0] = 0\n",
    "    \n",
    "    # Monitors G\n",
    "    stateMonGlobal = StateMonitor(G, 'v', record=True)\n",
    "    spikeMonGlobal = SpikeMonitor(G)\n",
    "    \n",
    "    # Synapsis definition\n",
    "    synConv1_Conv2 = Synapses(layerConv1, layerConv2, 'w : volt', on_pre='v_post += w')\n",
    "    synConv2_Dense = Synapses(layerConv2, layerDense, 'w : volt', on_pre='v_post += w')\n",
    "    \n",
    "    # Connect synapsis\n",
    "    synConv1_Conv2.connect(i=initSourceNeuronsConv1_Conv2, j=initTargetNeuronsConv1_Conv2)\n",
    "    synConv2_Dense.connect(i=initSourceNeuronsConv2_Dense, j=initTargetNeuronsConv2_Dense)\n",
    "    \n",
    "    synConv1_Conv2.w = norm_initWeightsConv1_Conv2*mV\n",
    "    synConv2_Dense.w = norm_initWeightsConv2_Dense*mV   \n",
    "    \n",
    "    # Invoke method only when the rat changes the position\n",
    "    @network_operation(dt=STEP_TIME)\n",
    "    def periodicFunction():\n",
    "        positionCounter = int(layerConv1.positionCounter[0])\n",
    "        #print(\"current_counter: \", positionCounter)\n",
    "        \n",
    "        if layerConv1.isFirstTime[0] == 1:\n",
    "            layerConv1.isFirstTime[0] = 0\n",
    "        else:\n",
    "            if positionCounter < len(list_coordinates_optimal_path):\n",
    "                # Reset all neurons to default I value\n",
    "                G.I = 10*mV/ms\n",
    "                \n",
    "                # Get the coordinates of the current position\n",
    "                coord = list_coordinates_optimal_path[positionCounter]\n",
    "\n",
    "                # Update I value only for the neurons related to the visible positions from the current one\n",
    "                list_neurons_index = get_related_neurons_visible_positions(coord[0], coord[1])\n",
    "\n",
    "                for neuron in list_neurons_index:\n",
    "                    G.I[neuron] = 15*mV/ms\n",
    "\n",
    "                # Update the counter of the current position over the maze (for next iteration)\n",
    "                layerConv1.positionCounter[0] += 1\n",
    "    \n",
    "    @network_operation(dt=1*ms)\n",
    "    def FLO():\n",
    "        if(params[\"attack\"] == \"FLO\"):\n",
    "            current_time_counter = int(layerConv1.timeCounter[0]*1000)\n",
    "            \n",
    "            # Allows repetitions of a FLO attack\n",
    "            current_times_attacks = []\n",
    "            \n",
    "            for instant in params[\"instants_attack\"]:\n",
    "                current_times_attacks.append(int(instant/ms))\n",
    "            \n",
    "            #print(\"------\")\n",
    "            #print(\"current_time_counter: \", current_time_counter)\n",
    "            #print(\"current_times_attacks: \", current_times_attacks)\n",
    "            \n",
    "            if(current_time_counter in current_times_attacks):\n",
    "                for neuron in params[\"neuron_list\"]:\n",
    "                    #print(\"neuron: \", neuron)\n",
    "                    #print(\"G.v[neuron]: \", G.v[neuron])\n",
    "\n",
    "                    layerConv1.v[neuron] += params[\"vIncrement\"]*mV\n",
    "                    \n",
    "                    add_result_simulation(results, neuron, current_time_counter)\n",
    "\n",
    "                    #print(\"NEW G.v[neuron]: \", G.v[neuron])\n",
    "                    #print(\"-------------------\")\n",
    "                    \n",
    "            layerConv1.timeCounter[0] += 1*ms #STEP_TIME\n",
    "            #print(\"AFTER layer1.timeCounter[0]: \", layer1.timeCounter[0])\n",
    "            #print(\"----------------------------\")        \n",
    "                        \n",
    "    @network_operation(dt=1*ms)\n",
    "    def JAM():\n",
    "        if(params[\"attack\"] == \"JAM\"):\n",
    "            current_time_counter = int(layerConv1.timeCounter[0]*1000)\n",
    "            \n",
    "            if (current_time_counter >= params[\"init_attack\"]/ms) & (current_time_counter <  params[\"end_attack\"]/ms):\n",
    "                for neuron in params[\"neuron_list\"]:\n",
    "                    layerConv1.v[neuron] = -65*mV\n",
    "                    \n",
    "                    add_result_simulation(results, neuron, current_time_counter)\n",
    "            \n",
    "            layerConv1.timeCounter[0] += 1*ms #STEP_TIME\n",
    "            \n",
    "    @network_operation(dt=1*ms)\n",
    "    def SCA_FOR():        \n",
    "        if(params[\"attack\"] == \"SCA\" or params[\"attack\"] == \"FOR\"):\n",
    "            neuron = int(layerConv1.neuronCounter[0])\n",
    "            \n",
    "            if (layerConv1.timeCounter[0] >= params[\"init_attack\"]):\n",
    "                if trunc(trunc((layerConv1.timeCounter[0]/ms)) % (params[\"steps_attack\"]/ms)) == 0:\n",
    "                    if(neuron < 200):\n",
    "                        \n",
    "                        #print(\"neuron: \", neuron)\n",
    "                        #print(\"G.v[neuron]: \", G.v[neuron])\n",
    "                        if params[\"attack\"] == \"SCA\": \n",
    "                            layerConv1.v[neuron] += params[\"vIncrement\"]*mV\n",
    "                            add_result_simulation(results, neuron, int(layerConv1.timeCounter[0]*1000))\n",
    "                        else: #FOR\n",
    "                            if (layerConv1.v[neuron] <= -65*mV): # Keep normal behavior\n",
    "                                pass\n",
    "                            elif (layerConv1.v[neuron] - params[\"vIncrement\"]*mV) >= -65*mV:\n",
    "                                layerConv1.v[neuron] -= params[\"vIncrement\"]*mV\n",
    "                                add_result_simulation(results, neuron, int(layerConv1.timeCounter[0]*1000))\n",
    "                            else:\n",
    "                                layerConv1.v[neuron] = -65*mV\n",
    "                                add_result_simulation(results, neuron, int(layerConv1.timeCounter[0]*1000))\n",
    "                        #print(\"NEW G.v[neuron]: \", G.v[neuron])\n",
    "                        #print(\"-------------------\")  \n",
    "                        \n",
    "                        \n",
    "                            \n",
    "                        layerConv1.neuronCounter[0] += 1\n",
    "                        \n",
    "            layerConv1.timeCounter[0] += 1*ms\n",
    "    \n",
    "    @network_operation(dt=0.1*ms)\n",
    "    def SPO():\n",
    "        if(params[\"attack\"] == \"SPO\"):\n",
    "            current_time_counter = round(layerConv1.timeCounter[0]/ms, 2)\n",
    "            #print(\"simulation instant: \", current_time_counter)\n",
    "            \n",
    "            if (current_time_counter >= params[\"init_attack\"]/ms) & (current_time_counter <  params[\"end_attack\"]/ms):\n",
    "                # Counter to determine the index within the lists to access the attack voltages\n",
    "                current_counter = int(layerConv1.neuronCounter[0])\n",
    "                #print(\"attack instant: \", current_counter)\n",
    "                \n",
    "                # Change the voltage of the targeted neurons according to current_counter\n",
    "                for neuron in params[\"neuron_list_attack\"]:\n",
    "                    #print(\"neuron: \", neuron)\n",
    "                    #print(\"G.v[neuron]: \", G.v[neuron])\n",
    "                    layerConv1.v[neuron] = params[\"recorded_activity\"][neuron][current_counter]\n",
    "                    #print(\"NEW G.v[neuron]: \", G.v[neuron])\n",
    "                    #print(\"-------------------\")\n",
    "                \n",
    "                layerConv1.neuronCounter[0] += 1\n",
    "            \n",
    "            layerConv1.timeCounter[0] += 0.1*ms #STEP_TIME\n",
    "        \n",
    "    @network_operation(dt=1*ms)\n",
    "    def SYB():\n",
    "        if(params[\"attack\"] == \"SYB\"):\n",
    "            current_time_counter = int(layerConv1.timeCounter[0]*1000)\n",
    "            \n",
    "            # Allows repetitions of a SYB attack\n",
    "            current_times_attacks = []\n",
    "            \n",
    "            for instant in params[\"instants_attack\"]:\n",
    "                current_times_attacks.append(int(instant/ms))\n",
    "            \n",
    "            if(current_time_counter in current_times_attacks):\n",
    "                #print(\"instant: \", current_time_counter)\n",
    "                for neuron in params[\"neuron_list\"]:\n",
    "                    #print(\"nNeuron: \", neuron)\n",
    "                    #print(\"layerConv1.v[neuron]: \", layerConv1.v[neuron])\n",
    "                    \n",
    "                    old_voltage = layerConv1.v[neuron]\n",
    "                    layerConv1.v[neuron] = resetValue*mV + thresholdValue*mV - layerConv1.v[neuron]\n",
    "                    #print(\"NEW layerConv1.v[neuron]: \", layerConv1.v[neuron])\n",
    "                    \n",
    "                    if layerConv1.v[neuron] < old_voltage:\n",
    "                        add_result_simulation(results[\"inhibition\"], neuron, current_time_counter)\n",
    "                    else:\n",
    "                        add_result_simulation(results[\"stimulation\"], neuron, current_time_counter)\n",
    "                    \n",
    "            layerConv1.timeCounter[0] += 1*ms #STEP_TIME\n",
    "        \n",
    "    \n",
    "    @network_operation(dt=1*ms)\n",
    "    def SIN():\n",
    "        if(params[\"attack\"] == \"SIN\"):\n",
    "            current_time_counter = int(layerConv1.timeCounter[0]*1000)\n",
    "            current_times_attacks = []\n",
    "            \n",
    "            for instant in params[\"instants_attack\"]:\n",
    "                current_times_attacks.append(int(instant/ms))\n",
    "            \n",
    "            if(current_time_counter in current_times_attacks):\n",
    "                for neuron in params[\"neuron_list\"]:\n",
    "                    layerConv1.v[neuron] += params[\"vIncrement\"]*mV\n",
    "                    \n",
    "                    add_result_simulation(results, neuron, current_time_counter)\n",
    "                    \n",
    "            layerConv1.timeCounter[0] += 1*ms #STEP_TIME\n",
    "            \n",
    "            \n",
    "    @network_operation(dt=1*ms)\n",
    "    def NON():\n",
    "        if(params[\"attack\"] == \"NON\"):           \n",
    "            if (layerConv1.timeCounter[0] >= params[\"init_attack\"]):\n",
    "                if trunc(trunc(layerConv1.timeCounter[0]/ms - params[\"init_attack\"]/ms) % (params[\"steps_attack\"]/ms)) == 0:\n",
    "                    #print(\"instant attack: \",layerConv1.timeCounter[0])\n",
    "                    for neuron in params[\"neuron_list\"]:\n",
    "                        action = random.choices([1,2,3], weights=(params[\"probability_spontaneous\"], \n",
    "                                                               params[\"probability_stimulation\"], \n",
    "                                                               params[\"probability_inhibition\"]), k=1)[0]\n",
    "                        \n",
    "                        #print(\"neuron: %s, action: %s\" %(neuron, action))\n",
    "                        if action == 2: # Stimulate\n",
    "                            #print(\"layerConv1.v[neuron]: \", layerConv1.v[neuron])\n",
    "                            layerConv1.v[neuron] += params[\"vIncrement\"]*mV\n",
    "                            add_result_simulation(results[\"stimulation\"], neuron, int(layerConv1.timeCounter[0]*1000))\n",
    "                            #print(\"NEW G.v[neuron]: \", G.v[neuron])\n",
    "                            #print(\"-------------------\")\n",
    "                        elif action == 3: # Inhibit\n",
    "                            #print(\"layerConv1.v[neuron]: \", layerConv1.v[neuron])\n",
    "                            if (layerConv1.v[neuron] <= -65*mV): # Keep normal behavior\n",
    "                                pass\n",
    "                            elif (layerConv1.v[neuron] - params[\"vIncrement\"]*mV) >= -65*mV:\n",
    "                                layerConv1.v[neuron] -= params[\"vIncrement\"]*mV\n",
    "                                add_result_simulation(results[\"inhibition\"], neuron, int(layerConv1.timeCounter[0]*1000))\n",
    "                            else:\n",
    "                                layerConv1.v[neuron] = -65*mV\n",
    "                                add_result_simulation(results[\"inhibition\"], neuron, int(layerConv1.timeCounter[0]*1000))\n",
    "                            #print(\"NEW G.v[neuron]: \", G.v[neuron])\n",
    "                            #print(\"-------------------\")\n",
    "                        \n",
    "            layerConv1.timeCounter[0] += 1*ms\n",
    "            \n",
    "    run(simulation_duration)\n",
    "    \n",
    "    return [spikeMonGlobal, stateMonGlobal]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for comparating between different combination of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_header_neurons_global = [\n",
    "    \"test\", \"n_attacks\", \"n_neurons\", \"stim_value\", \"vIncrement\", \"paramI\",\n",
    "    \"glbl_i_spikes_mean\",\"glbl_a_spikes_mean\",\"glbl_i_spikes_std\",\"glbl_a_spikes_std\",\"glbl_shifts_mean\",\n",
    "    \"attk_i_spikes_mean\",\"attk_a_spikes_mean\",\"attk_i_spikes_std\",\"attk_a_spikes_std\",\"attk_shifts_mean\",\n",
    "]\n",
    "\n",
    "csv_header_neurons_layers = [\n",
    "    \"test\", \"n_attacks\", \"n_neurons\", \"stim_value\", \"vIncrement\", \"paramI\", \"t_window\",\n",
    "    \"l1_glbl_i_spikes_mean\",\"l1_glbl_a_spikes_mean\",\"l1_glbl_i_spikes_std\",\"l1_glbl_a_spikes_std\",\"l1_glbl_shifts_mean\",\n",
    "    \"l1_attk_i_spikes_mean\",\"l1_attk_a_spikes_mean\",\"l1_attk_i_spikes_std\",\"l1_attk_a_spikes_std\",\"l1_attk_shifts_mean\",\n",
    "    \"l2_glbl_i_spikes_mean\",\"l2_glbl_a_spikes_mean\",\"l2_glbl_i_spikes_std\",\"l2_glbl_a_spikes_std\",\"l2_glbl_shifts_mean\",\n",
    "    \"l2_attk_i_spikes_mean\",\"l2_attk_a_spikes_mean\",\"l2_attk_i_spikes_std\",\"l2_attk_a_spikes_std\",\"l2_attk_shifts_mean\",\n",
    "    \"l3_glbl_i_spikes_mean\",\"l3_glbl_a_spikes_mean\",\"l3_glbl_i_spikes_std\",\"l3_glbl_a_spikes_std\",\"l3_glbl_shifts_mean\",\n",
    "    \"l3_attk_i_spikes_mean\",\"l3_attk_a_spikes_mean\",\"l3_attk_i_spikes_std\",\"l3_attk_a_spikes_std\",\"l3_attk_shifts_mean\",\n",
    "]\n",
    "\n",
    "# Stats files\n",
    "stats_dir_FLO = stats_dir+\"FLO/\"\n",
    "stats_dir_JAM = stats_dir+\"JAM/\"\n",
    "stats_dir_SCA = stats_dir+\"SCA/\"\n",
    "stats_dir_FOR = stats_dir+\"FOR/\"\n",
    "stats_dir_SPO = stats_dir+\"SPO/\"\n",
    "stats_dir_SYB = stats_dir+\"SYB/\"\n",
    "stats_dir_SIN = stats_dir+\"SIN/\"\n",
    "stats_dir_NON = stats_dir+\"NON/\"\n",
    "    \n",
    "csv_filename_FLO = stats_dir_FLO+\"temp_export_FLO.csv\"\n",
    "csv_filename_JAM = stats_dir_JAM+\"temp_export_JAM.csv\"\n",
    "csv_filename_SCA = stats_dir_SCA+\"temp_export_SCA.csv\"\n",
    "csv_filename_FOR = stats_dir_FOR+\"temp_export_FOR.csv\"\n",
    "csv_filename_SPO = stats_dir_SPO+\"temp_export_SPO.csv\"\n",
    "csv_filename_SYB = stats_dir_SYB+\"temp_export_SYB.csv\"\n",
    "csv_filename_SIN = stats_dir_SIN+\"temp_export_SIN.csv\"\n",
    "csv_filename_NON = stats_dir_NON+\"temp_export_NON.csv\"\n",
    "\n",
    "stats_neurons_global_csv_filename_FLO = stats_dir_FLO+\"stats_neurons_global_FLO.csv\"\n",
    "stats_neurons_layers_csv_filename_FLO = stats_dir_FLO+\"stats_neurons_layers_FLO.csv\"\n",
    "\n",
    "stats_neurons_global_csv_filename_JAM = stats_dir_JAM+\"stats_neurons_global_JAM.csv\"\n",
    "stats_neurons_layers_csv_filename_JAM = stats_dir_JAM+\"stats_neurons_layers_JAM.csv\"\n",
    "\n",
    "stats_neurons_global_csv_filename_SCA = stats_dir_SCA+\"stats_neurons_global_SCA.csv\"\n",
    "stats_neurons_layers_csv_filename_SCA = stats_dir_SCA+\"stats_neurons_layers_SCA.csv\"\n",
    "\n",
    "stats_neurons_global_csv_filename_FOR = stats_dir_FOR+\"stats_neurons_global_FOR.csv\"\n",
    "stats_neurons_layers_csv_filename_FOR = stats_dir_FOR+\"stats_neurons_layers_FOR.csv\"\n",
    "\n",
    "stats_neurons_global_csv_filename_SPO = stats_dir_SPO+\"stats_neurons_global_SPO.csv\"\n",
    "stats_neurons_layers_csv_filename_SPO = stats_dir_SPO+\"stats_neurons_layers_SPO.csv\"\n",
    "\n",
    "stats_neurons_global_csv_filename_SYB = stats_dir_SYB+\"stats_neurons_global_SYB.csv\"\n",
    "stats_neurons_layers_csv_filename_SYB = stats_dir_SYB+\"stats_neurons_layers_SYB.csv\"\n",
    "\n",
    "stats_neurons_global_csv_filename_SIN = stats_dir_SIN+\"stats_neurons_global_SIN.csv\"\n",
    "stats_neurons_layers_csv_filename_SIN = stats_dir_SIN+\"stats_neurons_layers_SIN.csv\"\n",
    "\n",
    "stats_neurons_global_csv_filename_NON = stats_dir_NON+\"stats_neurons_global_NON.csv\"\n",
    "stats_neurons_layers_csv_filename_NON = stats_dir_NON+\"stats_neurons_layers_NON.csv\"\n",
    "\n",
    "stats_dispersion_csv_filename_FLO = stats_dir_FLO+\"stats_dispersion_FLO.csv\"\n",
    "stats_dispersion_csv_filename_JAM = stats_dir_JAM+\"stats_dispersion_JAM.csv\"\n",
    "stats_dispersion_csv_filename_SCA = stats_dir_SCA+\"stats_dispersion_SCA.csv\"\n",
    "stats_dispersion_csv_filename_FOR = stats_dir_FOR+\"stats_dispersion_FOR.csv\"\n",
    "stats_dispersion_csv_filename_SPO = stats_dir_SPO+\"stats_dispersion_SPO.csv\"\n",
    "stats_dispersion_csv_filename_SYB = stats_dir_SYB+\"stats_dispersion_SYB.csv\"\n",
    "stats_dispersion_csv_filename_SIN = stats_dir_SIN+\"stats_dispersion_SIN.csv\"\n",
    "stats_dispersion_csv_filename_NON = stats_dir_NON+\"stats_dispersion_NON.csv\"\n",
    "\n",
    "output_file_FLO = stats_dir_FLO+\"output_moving_FLO.txt\"\n",
    "output_file_JAM = stats_dir_JAM+\"output_moving_JAM.txt\"\n",
    "output_file_SCA = stats_dir_SCA+\"output_moving_SCA.txt\"\n",
    "output_file_FOR = stats_dir_FOR+\"output_moving_FOR.txt\"\n",
    "output_file_SPO = stats_dir_SPO+\"output_moving_SPO.txt\"\n",
    "output_file_SYB = stats_dir_SYB+\"output_moving_SYB.txt\"\n",
    "output_file_SIN = stats_dir_SIN+\"output_moving_SIN.txt\"\n",
    "output_file_NON = stats_dir_NON+\"output_moving_NON.txt\"\n",
    "\n",
    "# Plots\n",
    "plots_dir_FLO_raster = plots_dir+\"FLO/rasters/\"\n",
    "plots_dir_FLO_spikes = plots_dir+\"FLO/spikes/\"\n",
    "plots_dir_FLO_dispersion = plots_dir+\"FLO/dispersion/\"\n",
    "plots_dir_FLO_shifts = plots_dir+\"FLO/shifts/\"\n",
    "\n",
    "plots_dir_JAM_raster = plots_dir+\"JAM/rasters/\"\n",
    "plots_dir_JAM_spikes = plots_dir+\"JAM/spikes/\"\n",
    "plots_dir_JAM_dispersion = plots_dir+\"JAM/dispersion/\"\n",
    "plots_dir_JAM_shifts = plots_dir+\"JAM/shifts/\"\n",
    "\n",
    "plots_dir_SCA_raster = plots_dir+\"SCA/rasters/\"\n",
    "plots_dir_SCA_spikes = plots_dir+\"SCA/spikes/\"\n",
    "plots_dir_SCA_dispersion = plots_dir+\"SCA/dispersion/\"\n",
    "plots_dir_SCA_shifts = plots_dir+\"SCA/shifts/\"\n",
    "\n",
    "plots_dir_FOR_raster = plots_dir+\"FOR/rasters/\"\n",
    "plots_dir_FOR_spikes = plots_dir+\"FOR/spikes/\"\n",
    "plots_dir_FOR_dispersion = plots_dir+\"FOR/dispersion/\"\n",
    "plots_dir_FOR_shifts = plots_dir+\"FOR/shifts/\"\n",
    "\n",
    "plots_dir_SPO_raster = plots_dir+\"SPO/rasters/\"\n",
    "plots_dir_SPO_spikes = plots_dir+\"SPO/spikes/\"\n",
    "plots_dir_SPO_dispersion = plots_dir+\"SPO/dispersion/\"\n",
    "plots_dir_SPO_shifts = plots_dir+\"SPO/shifts/\"\n",
    "\n",
    "plots_dir_SYB_raster = plots_dir+\"SYB/rasters/\"\n",
    "plots_dir_SYB_spikes = plots_dir+\"SYB/spikes/\"\n",
    "plots_dir_SYB_dispersion = plots_dir+\"SYB/dispersion/\"\n",
    "plots_dir_SYB_shifts = plots_dir+\"SYB/shifts/\"\n",
    "\n",
    "plots_dir_SIN_raster = plots_dir+\"SIN/rasters/\"\n",
    "plots_dir_SIN_spikes = plots_dir+\"SIN/spikes/\"\n",
    "plots_dir_SIN_dispersion = plots_dir+\"SIN/dispersion/\"\n",
    "plots_dir_SIN_shifts = plots_dir+\"SIN/shifts/\"\n",
    "\n",
    "plots_dir_NON_raster = plots_dir+\"NON/rasters/\"\n",
    "plots_dir_NON_spikes = plots_dir+\"NON/spikes/\"\n",
    "plots_dir_NON_dispersion = plots_dir+\"NON/dispersion/\"\n",
    "plots_dir_NON_shifts = plots_dir+\"NON/shifts/\"\n",
    "\n",
    "# Raster plots\n",
    "csv_rasters_FLO = stats_dir_FLO+\"rasters_FLO.csv\"\n",
    "csv_rasters_JAM = stats_dir_JAM+\"rasters_JAM.csv\"\n",
    "csv_rasters_SCA = stats_dir_SCA+\"rasters_SCA.csv\"\n",
    "csv_rasters_FOR = stats_dir_FOR+\"rasters_FOR.csv\"\n",
    "csv_rasters_SPO = stats_dir_SPO+\"rasters_SPO.csv\"\n",
    "csv_rasters_SYB = stats_dir_SYB+\"rasters_SYB.csv\"\n",
    "csv_rasters_SIN = stats_dir_SIN+\"rasters_SIN.csv\"\n",
    "csv_rasters_NON = stats_dir_NON+\"rasters_NON.csv\"\n",
    "\n",
    "# Raster aggregated (\"cloud of dots\") for each position\n",
    "aggr_csv_filename_FLO = stats_dir_FLO+\"aggr_FLO.csv\"\n",
    "aggr_csv_filename_JAM = stats_dir_JAM+\"aggr_JAM.csv\"\n",
    "aggr_csv_filename_SCA = stats_dir_SCA+\"aggr_SCA.csv\"\n",
    "aggr_csv_filename_FOR = stats_dir_FOR+\"aggr_FOR.csv\"\n",
    "aggr_csv_filename_SPO = stats_dir_SPO+\"aggr_SPO.csv\"\n",
    "aggr_csv_filename_SYB = stats_dir_SYB+\"aggr_SYB.csv\"\n",
    "aggr_csv_filename_SIN = stats_dir_SIN+\"aggr_SIN.csv\"\n",
    "aggr_csv_filename_NON = stats_dir_NON+\"aggr_NON.csv\"\n",
    "\n",
    "condition_generate_rasters = {\n",
    "    \"n_neurons\": [100],\n",
    "    \"v_increment\": [40],\n",
    "}\n",
    "\n",
    "T1_init = 0\n",
    "T1_fin = 1000\n",
    "\n",
    "T2_init = 12000\n",
    "T2_fin = 13000\n",
    "\n",
    "T3_init = 26000\n",
    "T3_fin = 27000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they do not exist\n",
    "\n",
    "if \"FLO\" in list_attack_generation:    \n",
    "    # Stats\n",
    "    if not os.path.exists(stats_dir_FLO):\n",
    "        os.makedirs(stats_dir_FLO)\n",
    "        \n",
    "    # Plots\n",
    "    if not os.path.exists(plots_dir_FLO_raster):\n",
    "        os.makedirs(plots_dir_FLO_raster)\n",
    "\n",
    "    if not os.path.exists(plots_dir_FLO_spikes):\n",
    "        os.makedirs(plots_dir_FLO_spikes)\n",
    "\n",
    "    if not os.path.exists(plots_dir_FLO_dispersion):\n",
    "        os.makedirs(plots_dir_FLO_dispersion)\n",
    "\n",
    "    if not os.path.exists(plots_dir_FLO_shifts):\n",
    "        os.makedirs(plots_dir_FLO_shifts)\n",
    "\n",
    "if \"JAM\" in list_attack_generation:    \n",
    "    # Stats\n",
    "    if not os.path.exists(stats_dir_JAM):\n",
    "        os.makedirs(stats_dir_JAM)\n",
    "        \n",
    "    # Plots\n",
    "    if not os.path.exists(plots_dir_JAM_raster):\n",
    "        os.makedirs(plots_dir_JAM_raster)\n",
    "\n",
    "    if not os.path.exists(plots_dir_JAM_spikes):\n",
    "        os.makedirs(plots_dir_JAM_spikes)\n",
    "\n",
    "    if not os.path.exists(plots_dir_JAM_dispersion):\n",
    "        os.makedirs(plots_dir_JAM_dispersion)\n",
    "\n",
    "    if not os.path.exists(plots_dir_JAM_shifts):\n",
    "        os.makedirs(plots_dir_JAM_shifts)\n",
    "\n",
    "if \"SCA\" in list_attack_generation:\n",
    "    # Stats\n",
    "    if not os.path.exists(stats_dir_SCA):\n",
    "        os.makedirs(stats_dir_SCA)\n",
    "\n",
    "    # Plots\n",
    "    if not os.path.exists(plots_dir_SCA_raster):\n",
    "        os.makedirs(plots_dir_SCA_raster)\n",
    "\n",
    "    if not os.path.exists(plots_dir_SCA_spikes):\n",
    "        os.makedirs(plots_dir_SCA_spikes)\n",
    "\n",
    "    if not os.path.exists(plots_dir_SCA_dispersion):\n",
    "        os.makedirs(plots_dir_SCA_dispersion)\n",
    "\n",
    "    if not os.path.exists(plots_dir_SCA_shifts):\n",
    "        os.makedirs(plots_dir_SCA_shifts)\n",
    "        \n",
    "if \"FOR\" in list_attack_generation:    \n",
    "    # Stats\n",
    "    if not os.path.exists(stats_dir_FOR):\n",
    "        os.makedirs(stats_dir_FOR)\n",
    "        \n",
    "    # Plots\n",
    "    if not os.path.exists(plots_dir_FOR_raster):\n",
    "        os.makedirs(plots_dir_FOR_raster)\n",
    "\n",
    "    if not os.path.exists(plots_dir_FOR_spikes):\n",
    "        os.makedirs(plots_dir_FOR_spikes)\n",
    "\n",
    "    if not os.path.exists(plots_dir_FOR_dispersion):\n",
    "        os.makedirs(plots_dir_FOR_dispersion)\n",
    "\n",
    "    if not os.path.exists(plots_dir_FOR_shifts):\n",
    "        os.makedirs(plots_dir_FOR_shifts)\n",
    "        \n",
    "if \"SPO\" in list_attack_generation:    \n",
    "    # Stats\n",
    "    if not os.path.exists(stats_dir_SPO):\n",
    "        os.makedirs(stats_dir_SPO)\n",
    "        \n",
    "    # Plots\n",
    "    if not os.path.exists(plots_dir_SPO_raster):\n",
    "        os.makedirs(plots_dir_SPO_raster)\n",
    "\n",
    "    if not os.path.exists(plots_dir_SPO_spikes):\n",
    "        os.makedirs(plots_dir_SPO_spikes)\n",
    "\n",
    "    if not os.path.exists(plots_dir_SPO_dispersion):\n",
    "        os.makedirs(plots_dir_SPO_dispersion)\n",
    "\n",
    "    if not os.path.exists(plots_dir_SPO_shifts):\n",
    "        os.makedirs(plots_dir_SPO_shifts)\n",
    "        \n",
    "if \"SYB\" in list_attack_generation:    \n",
    "    # Stats\n",
    "    if not os.path.exists(stats_dir_SYB):\n",
    "        os.makedirs(stats_dir_SYB)\n",
    "        \n",
    "    # Plots\n",
    "    if not os.path.exists(plots_dir_SYB_raster):\n",
    "        os.makedirs(plots_dir_SYB_raster)\n",
    "\n",
    "    if not os.path.exists(plots_dir_SYB_spikes):\n",
    "        os.makedirs(plots_dir_SYB_spikes)\n",
    "\n",
    "    if not os.path.exists(plots_dir_SYB_dispersion):\n",
    "        os.makedirs(plots_dir_SYB_dispersion)\n",
    "\n",
    "    if not os.path.exists(plots_dir_SYB_shifts):\n",
    "        os.makedirs(plots_dir_SYB_shifts)\n",
    "        \n",
    "if \"SIN\" in list_attack_generation:    \n",
    "    # Stats\n",
    "    if not os.path.exists(stats_dir_SIN):\n",
    "        os.makedirs(stats_dir_SIN)\n",
    "        \n",
    "    # Plots\n",
    "    if not os.path.exists(plots_dir_SIN_raster):\n",
    "        os.makedirs(plots_dir_SIN_raster)\n",
    "\n",
    "    if not os.path.exists(plots_dir_SIN_spikes):\n",
    "        os.makedirs(plots_dir_SIN_spikes)\n",
    "\n",
    "    if not os.path.exists(plots_dir_SIN_dispersion):\n",
    "        os.makedirs(plots_dir_SIN_dispersion)\n",
    "\n",
    "    if not os.path.exists(plots_dir_SIN_shifts):\n",
    "        os.makedirs(plots_dir_SIN_shifts)\n",
    "        \n",
    "if \"NON\" in list_attack_generation:    \n",
    "    # Stats\n",
    "    if not os.path.exists(stats_dir_NON):\n",
    "        os.makedirs(stats_dir_NON)\n",
    "        \n",
    "    # Plots\n",
    "    if not os.path.exists(plots_dir_NON_raster):\n",
    "        os.makedirs(plots_dir_NON_raster)\n",
    "\n",
    "    if not os.path.exists(plots_dir_NON_spikes):\n",
    "        os.makedirs(plots_dir_NON_spikes)\n",
    "\n",
    "    if not os.path.exists(plots_dir_NON_dispersion):\n",
    "        os.makedirs(plots_dir_NON_dispersion)\n",
    "\n",
    "    if not os.path.exists(plots_dir_NON_shifts):\n",
    "        os.makedirs(plots_dir_NON_shifts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"FLO\" in list_attack_generation:\n",
    "    open(csv_filename_FLO, 'w').close() # Remove file content\n",
    "    append_to_csv_file(csv_filename_FLO, [\"attack\", \"test\", \"position_attack\", \"n_attacks\", \"n_neurons\", \"attacked_neurons\", \"coord_attack\", \"stim_value\", \"n_exec\", \"vIncrement\", \"paramI\", \"time_delta\", \"neuron\"])\n",
    "    \n",
    "    # Overwrite raster files\n",
    "    open(csv_rasters_FLO, 'w').close() # Remove file content\n",
    "    append_to_csv_file(csv_rasters_FLO, [\"attack\", \"test\", \"position_attack\", \"n_attacks\", \"n_neurons\", \"attacked_neurons\", \"coord_attack\", \"stim_value\", \"n_exec\", \"vIncrement\", \"paramI\", \"time_delta\", \"neuron\"])\n",
    "    \n",
    "    # Overwrite raster aggregated files\n",
    "    open(aggr_csv_filename_FLO, 'w').close() # Remove file content\n",
    "    append_to_csv_file(aggr_csv_filename_FLO, [\"time_delta\", \"number_spikes\", \"position_attack\", \"attack\", \"position\", \"n_neurons\", \"v_increment\", \"n_exec\"])\n",
    "\n",
    "    # Overwrite stats neurons CSV\n",
    "    open(stats_neurons_global_csv_filename_FLO, 'w').close() # Remove file content\n",
    "    append_to_csv_file(stats_neurons_global_csv_filename_FLO, csv_header_neurons_global)\n",
    "\n",
    "    open(stats_neurons_layers_csv_filename_FLO, 'w').close() # Remove file content\n",
    "    append_to_csv_file(stats_neurons_layers_csv_filename_FLO, csv_header_neurons_layers)\n",
    "    \n",
    "    # Output file\n",
    "    open(output_file_FLO, 'w').close() # Remove file content\n",
    "    \n",
    "if \"SCA\" in list_attack_generation:\n",
    "    open(csv_filename_SCA, 'w').close() # Remove file content\n",
    "    append_to_csv_file(csv_filename_SCA, [\"attack\", \"test\", \"position_attack\", \"n_attacks\", \"n_neurons\", \"attacked_neurons\", \"coord_attack\", \"stim_value\", \"n_exec\", \"vIncrement\", \"paramI\", \"time_delta\", \"neuron\"])\n",
    "\n",
    "    # Overwrite raster files\n",
    "    open(csv_rasters_SCA, 'w').close() # Remove file content\n",
    "    append_to_csv_file(csv_rasters_SCA, [\"attack\", \"test\", \"position_attack\", \"n_attacks\", \"n_neurons\", \"attacked_neurons\", \"coord_attack\", \"stim_value\", \"n_exec\", \"vIncrement\", \"paramI\", \"time_delta\", \"neuron\"])\n",
    "\n",
    "    # Overwrite raster aggregated files\n",
    "    open(aggr_csv_filename_SCA, 'w').close() # Remove file content\n",
    "    append_to_csv_file(aggr_csv_filename_SCA, [\"time_delta\", \"number_spikes\", \"position_attack\", \"attack\", \"position\", \"n_neurons\", \"v_increment\", \"n_exec\"])\n",
    "    \n",
    "    # Overwrite stats neurons CSV\n",
    "    open(stats_neurons_global_csv_filename_SCA, 'w').close() # Remove file content\n",
    "    append_to_csv_file(stats_neurons_global_csv_filename_SCA, csv_header_neurons_global)\n",
    "\n",
    "    open(stats_neurons_layers_csv_filename_SCA, 'w').close() # Remove file content\n",
    "    append_to_csv_file(stats_neurons_layers_csv_filename_SCA, csv_header_neurons_layers)\n",
    "    \n",
    "    # Output file\n",
    "    open(output_file_SCA, 'w').close() # Remove file content\n",
    "\n",
    "if \"JAM\" in list_attack_generation:\n",
    "    open(csv_filename_JAM, 'w').close() # Remove file content\n",
    "    append_to_csv_file(csv_filename_JAM, [\"attack\", \"test\", \"position_attack\", \"n_attacks\", \"n_neurons\", \"attacked_neurons\", \"coord_attack\", \"stim_value\", \"n_exec\", \"vIncrement\", \"paramI\", \"time_delta\", \"neuron\"])\n",
    "    \n",
    "    # Overwrite raster files\n",
    "    open(csv_rasters_JAM, 'w').close() # Remove file content\n",
    "    append_to_csv_file(csv_rasters_JAM, [\"attack\", \"test\", \"position_attack\", \"n_attacks\", \"n_neurons\", \"attacked_neurons\", \"coord_attack\", \"stim_value\", \"n_exec\", \"vIncrement\", \"paramI\", \"time_delta\", \"neuron\"])\n",
    "    \n",
    "    # Overwrite raster aggregated files\n",
    "    open(aggr_csv_filename_JAM, 'w').close() # Remove file content\n",
    "    append_to_csv_file(aggr_csv_filename_JAM, [\"time_delta\", \"number_spikes\", \"position_attack\", \"attack\", \"position\", \"n_neurons\", \"v_increment\", \"n_exec\"])\n",
    "\n",
    "    # Overwrite stats neurons CSV\n",
    "    open(stats_neurons_global_csv_filename_JAM, 'w').close() # Remove file content\n",
    "    append_to_csv_file(stats_neurons_global_csv_filename_JAM, csv_header_neurons_global)\n",
    "\n",
    "    open(stats_neurons_layers_csv_filename_JAM, 'w').close() # Remove file content\n",
    "    append_to_csv_file(stats_neurons_layers_csv_filename_JAM, csv_header_neurons_layers)\n",
    "    \n",
    "    # Output file\n",
    "    open(output_file_JAM, 'w').close() # Remove file content\n",
    "    \n",
    "if \"FOR\" in list_attack_generation:\n",
    "    open(csv_filename_FOR, 'w').close() # Remove file content\n",
    "    append_to_csv_file(csv_filename_FOR, [\"attack\", \"test\", \"position_attack\", \"n_attacks\", \"n_neurons\", \"attacked_neurons\", \"coord_attack\", \"stim_value\", \"n_exec\", \"vIncrement\", \"paramI\", \"time_delta\", \"neuron\"])\n",
    "    \n",
    "    # Overwrite raster files\n",
    "    open(csv_rasters_FOR, 'w').close() # Remove file content\n",
    "    append_to_csv_file(csv_rasters_FOR, [\"attack\", \"test\", \"position_attack\", \"n_attacks\", \"n_neurons\", \"attacked_neurons\", \"coord_attack\", \"stim_value\", \"n_exec\", \"vIncrement\", \"paramI\", \"time_delta\", \"neuron\"])\n",
    "    \n",
    "    # Overwrite raster aggregated files\n",
    "    open(aggr_csv_filename_FOR, 'w').close() # Remove file content\n",
    "    append_to_csv_file(aggr_csv_filename_FOR, [\"time_delta\", \"number_spikes\", \"position_attack\", \"attack\", \"position\", \"n_neurons\", \"v_increment\", \"n_exec\"])\n",
    "\n",
    "    # Overwrite stats neurons CSV\n",
    "    open(stats_neurons_global_csv_filename_FOR, 'w').close() # Remove file content\n",
    "    append_to_csv_file(stats_neurons_global_csv_filename_FOR, csv_header_neurons_global)\n",
    "\n",
    "    open(stats_neurons_layers_csv_filename_FOR, 'w').close() # Remove file content\n",
    "    append_to_csv_file(stats_neurons_layers_csv_filename_FOR, csv_header_neurons_layers)\n",
    "    \n",
    "    # Output file\n",
    "    open(output_file_FOR, 'w').close() # Remove file content\n",
    "    \n",
    "if \"SPO\" in list_attack_generation:\n",
    "    open(csv_filename_SPO, 'w').close() # Remove file content\n",
    "    append_to_csv_file(csv_filename_SPO, [\"attack\", \"test\", \"position_attack\", \"n_attacks\", \"n_neurons\", \"attacked_neurons\", \"coord_attack\", \"stim_value\", \"n_exec\", \"vIncrement\", \"paramI\", \"time_delta\", \"neuron\"])\n",
    "    \n",
    "    # Overwrite raster files\n",
    "    open(csv_rasters_SPO, 'w').close() # Remove file content\n",
    "    append_to_csv_file(csv_rasters_SPO, [\"attack\", \"test\", \"position_attack\", \"n_attacks\", \"n_neurons\", \"attacked_neurons\", \"coord_attack\", \"stim_value\", \"n_exec\", \"vIncrement\", \"paramI\", \"time_delta\", \"neuron\"])\n",
    "    \n",
    "    # Overwrite raster aggregated files\n",
    "    open(aggr_csv_filename_SPO, 'w').close() # Remove file content\n",
    "    append_to_csv_file(aggr_csv_filename_SPO, [\"time_delta\", \"number_spikes\", \"position_attack\", \"attack\", \"position\", \"n_neurons\", \"v_increment\", \"n_exec\"])\n",
    "\n",
    "    # Overwrite stats neurons CSV\n",
    "    open(stats_neurons_global_csv_filename_SPO, 'w').close() # Remove file content\n",
    "    append_to_csv_file(stats_neurons_global_csv_filename_SPO, csv_header_neurons_global)\n",
    "\n",
    "    open(stats_neurons_layers_csv_filename_SPO, 'w').close() # Remove file content\n",
    "    append_to_csv_file(stats_neurons_layers_csv_filename_SPO, csv_header_neurons_layers)\n",
    "    \n",
    "    # Output file\n",
    "    open(output_file_SPO, 'w').close() # Remove file content\n",
    "    \n",
    "if \"SYB\" in list_attack_generation:\n",
    "    open(csv_filename_SYB, 'w').close() # Remove file content\n",
    "    append_to_csv_file(csv_filename_SYB, [\"attack\", \"test\", \"position_attack\", \"n_attacks\", \"n_neurons\", \"attacked_neurons\", \"coord_attack\", \"stim_value\", \"n_exec\", \"vIncrement\", \"paramI\", \"time_delta\", \"neuron\"])\n",
    "    \n",
    "    # Overwrite raster files\n",
    "    open(csv_rasters_SYB, 'w').close() # Remove file content\n",
    "    append_to_csv_file(csv_rasters_SYB, [\"attack\", \"test\", \"position_attack\", \"n_attacks\", \"n_neurons\", \"attacked_neurons\", \"coord_attack\", \"stim_value\", \"n_exec\", \"vIncrement\", \"paramI\", \"time_delta\", \"neuron\"])\n",
    "    \n",
    "    # Overwrite raster aggregated files\n",
    "    open(aggr_csv_filename_SYB, 'w').close() # Remove file content\n",
    "    append_to_csv_file(aggr_csv_filename_SYB, [\"time_delta\", \"number_spikes\", \"position_attack\", \"attack\", \"position\", \"n_neurons\", \"v_increment\", \"n_exec\"])\n",
    "\n",
    "    # Overwrite stats neurons CSV\n",
    "    open(stats_neurons_global_csv_filename_SYB, 'w').close() # Remove file content\n",
    "    append_to_csv_file(stats_neurons_global_csv_filename_SYB, csv_header_neurons_global)\n",
    "\n",
    "    open(stats_neurons_layers_csv_filename_SYB, 'w').close() # Remove file content\n",
    "    append_to_csv_file(stats_neurons_layers_csv_filename_SYB, csv_header_neurons_layers)\n",
    "    \n",
    "    # Output file\n",
    "    open(output_file_SYB, 'w').close() # Remove file content\n",
    "    \n",
    "if \"SIN\" in list_attack_generation:\n",
    "    open(csv_filename_SIN, 'w').close() # Remove file content\n",
    "    append_to_csv_file(csv_filename_SIN, [\"attack\", \"test\", \"position_attack\", \"n_attacks\", \"n_neurons\", \"attacked_neurons\", \"coord_attack\", \"stim_value\", \"n_exec\", \"vIncrement\", \"paramI\", \"time_delta\", \"neuron\"])\n",
    "    \n",
    "    # Overwrite raster files\n",
    "    open(csv_rasters_SIN, 'w').close() # Remove file content\n",
    "    append_to_csv_file(csv_rasters_SIN, [\"attack\", \"test\", \"position_attack\", \"n_attacks\", \"n_neurons\", \"attacked_neurons\", \"coord_attack\", \"stim_value\", \"n_exec\", \"vIncrement\", \"paramI\", \"time_delta\", \"neuron\"])\n",
    "    \n",
    "    # Overwrite raster aggregated files\n",
    "    open(aggr_csv_filename_SIN, 'w').close() # Remove file content\n",
    "    append_to_csv_file(aggr_csv_filename_SIN, [\"time_delta\", \"number_spikes\", \"position_attack\", \"attack\", \"position\", \"n_neurons\", \"v_increment\", \"n_exec\"])\n",
    "\n",
    "    # Overwrite stats neurons CSV\n",
    "    open(stats_neurons_global_csv_filename_SIN, 'w').close() # Remove file content\n",
    "    append_to_csv_file(stats_neurons_global_csv_filename_SIN, csv_header_neurons_global)\n",
    "\n",
    "    open(stats_neurons_layers_csv_filename_SIN, 'w').close() # Remove file content\n",
    "    append_to_csv_file(stats_neurons_layers_csv_filename_SIN, csv_header_neurons_layers)\n",
    "    \n",
    "    # Output file\n",
    "    open(output_file_SIN, 'w').close() # Remove file content\n",
    "    \n",
    "if \"NON\" in list_attack_generation:\n",
    "    open(csv_filename_NON, 'w').close() # Remove file content\n",
    "    append_to_csv_file(csv_filename_NON, [\"attack\", \"test\", \"position_attack\", \"n_attacks\", \"n_neurons\", \"attacked_neurons\", \"coord_attack\", \"stim_value\", \"n_exec\", \"vIncrement\", \"paramI\", \"time_delta\", \"neuron\"])\n",
    "    \n",
    "    # Overwrite raster files\n",
    "    open(csv_rasters_NON, 'w').close() # Remove file content\n",
    "    append_to_csv_file(csv_rasters_NON, [\"attack\", \"test\", \"position_attack\", \"n_attacks\", \"n_neurons\", \"attacked_neurons\", \"coord_attack\", \"stim_value\", \"n_exec\", \"vIncrement\", \"paramI\", \"time_delta\", \"neuron\"])\n",
    "    \n",
    "    # Overwrite raster aggregated files\n",
    "    open(aggr_csv_filename_NON, 'w').close() # Remove file content\n",
    "    append_to_csv_file(aggr_csv_filename_NON, [\"time_delta\", \"number_spikes\", \"position_attack\", \"attack\", \"position\", \"n_neurons\", \"v_increment\", \"n_exec\"])\n",
    "\n",
    "    # Overwrite stats neurons CSV\n",
    "    open(stats_neurons_global_csv_filename_NON, 'w').close() # Remove file content\n",
    "    append_to_csv_file(stats_neurons_global_csv_filename_NON, csv_header_neurons_global)\n",
    "\n",
    "    open(stats_neurons_layers_csv_filename_NON, 'w').close() # Remove file content\n",
    "    append_to_csv_file(stats_neurons_layers_csv_filename_NON, csv_header_neurons_layers)\n",
    "    \n",
    "    # Output file\n",
    "    open(output_file_NON, 'w').close() # Remove file content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stats_neurons_pandas(test_id, position_attack, attack, csv_filename, stats_neurons_global_csv_filename, stats_neurons_layers_csv_filename):\n",
    "    df = pd.read_csv(csv_filename, delimiter=\";\", dtype={'n_neurons': np.int32})\n",
    "    \n",
    "    #### 1) Aggregated statistics for all layers (global)\n",
    "    data_initial_state = df[df.attack == \"initial_state\"]\n",
    "    df_attack = df[df.attack == attack]\n",
    "    \n",
    "    list_attacked_neurons = df_attack[\"attacked_neurons\"].unique()[0].split(\"-\")\n",
    "    \n",
    "    grouped = df_attack.groupby('neuron').count()[\"time_delta\"]\n",
    "    d_attack_aggr_neuron = pd.DataFrame()\n",
    "    d_attack_aggr_neuron['neuron'] = grouped.index\n",
    "    d_attack_aggr_neuron['n_spikes'] = grouped.tolist()\n",
    "    \n",
    "    # Get the data from the initial_state experiment based on the paramI value\n",
    "    param_I = df_attack.paramI.unique()[0]\n",
    "    \n",
    "    grouped = data_initial_state[(data_initial_state.paramI == param_I)].groupby('neuron').count()[\"time_delta\"]\n",
    "    d_initial_aggr_neuron = pd.DataFrame()\n",
    "    d_initial_aggr_neuron['neuron'] = grouped.index\n",
    "    d_initial_aggr_neuron['n_spikes'] = grouped.tolist()\n",
    "    \n",
    "    data_csv = []\n",
    "    data_csv.append(test_id)\n",
    "    data_csv.append(position_attack)\n",
    "    data_csv.append(df_attack[\"n_attacks\"].unique()[0])\n",
    "    data_csv.append(df_attack[\"n_neurons\"].unique()[0])\n",
    "    data_csv.append(df_attack[\"stim_value\"].unique()[0])\n",
    "    data_csv.append(df_attack[\"vIncrement\"].unique()[0])\n",
    "    data_csv.append(df_attack[\"paramI\"].unique()[0])   \n",
    "    \n",
    "    ### GLOBAL PERSPECTIVE\n",
    "    # Total number of spikes\n",
    "    initial_nSpikes = data_initial_state[(data_initial_state.paramI == param_I)].count()[\"neuron\"]\n",
    "    attack_nSpikes = df_attack.count()[\"neuron\"]\n",
    "\n",
    "    # Mean of spikes (over the total neurons of the layer)\n",
    "    initial_mean_spikes = round(d_initial_aggr_neuron[\"n_spikes\"].mean(), 2)\n",
    "    attack_mean_spikes = round(d_attack_aggr_neuron[\"n_spikes\"].mean(), 2)\n",
    "\n",
    "    # Standard deviation (over the total neurons of the layer)\n",
    "    initial_std_spikes = round(d_initial_aggr_neuron[\"n_spikes\"].std(), 2)\n",
    "    attack_std_spikes = round(d_attack_aggr_neuron[\"n_spikes\"].std(), 2)\n",
    "\n",
    "    # Percentage of shifts between the attack and normal behaviour\n",
    "    initial_nSpikes_join = data_initial_state[(data_initial_state.paramI == param_I)][[\"attack\", \"time_delta\", \"neuron\"]].copy()\n",
    "    attack_nSpikes_join = df_attack[[\"attack\", \"time_delta\", \"neuron\"]].copy()\n",
    "    join_initial_attack_shifts = pd.merge(attack_nSpikes_join, initial_nSpikes_join,  how='left', left_on=['time_delta','neuron'], right_on = ['time_delta','neuron'])\n",
    "    join_initial_attack_shifts = join_initial_attack_shifts.fillna(0)\n",
    "    n_shifts_initial_attack = join_initial_attack_shifts[join_initial_attack_shifts.attack_y == 0].count()[\"neuron\"]\n",
    "    percent_shifts_initial_attack = round((n_shifts_initial_attack/attack_nSpikes)*100,2)\n",
    "\n",
    "    # List of neurons shifted in this layer\n",
    "    attack_shifted_neurons = join_initial_attack_shifts[join_initial_attack_shifts.attack_y == 0][\"neuron\"].unique()\n",
    "\n",
    "    ### ATTACKED NEURONS PERSPECTIVE\n",
    "\n",
    "    # spikes: mean\n",
    "    a_initial_mean_spikes = round(d_initial_aggr_neuron[(d_initial_aggr_neuron.neuron.isin(attack_shifted_neurons))][\"n_spikes\"].mean(), 2)\n",
    "    a_attack_mean_spikes = round(d_attack_aggr_neuron[(d_attack_aggr_neuron.neuron.isin(attack_shifted_neurons))][\"n_spikes\"].mean(), 2)\n",
    "\n",
    "    # Standard deviation\n",
    "    a_initial_std_spikes = round(d_initial_aggr_neuron[(d_initial_aggr_neuron.neuron.isin(attack_shifted_neurons))][\"n_spikes\"].std(), 2)\n",
    "    a_attack_std_spikes = round(d_attack_aggr_neuron[(d_attack_aggr_neuron.neuron.isin(attack_shifted_neurons))][\"n_spikes\"].std(), 2)\n",
    "\n",
    "    # mean of % shifts\n",
    "    a_join_shifts_attk = join_initial_attack_shifts[join_initial_attack_shifts.neuron.isin(attack_shifted_neurons)]\n",
    "    a_n_shifts_zeros = a_join_shifts_attk[a_join_shifts_attk.attack_y == 0].count()[\"neuron\"]\n",
    "    # number of shifts under attack (count zeros) / total number of spikes under attack \n",
    "    a_mean_shifts = round((a_n_shifts_zeros/a_join_shifts_attk.neuron.count())*100, 2)\n",
    "\n",
    "    data_csv.append(initial_mean_spikes)\n",
    "    data_csv.append(attack_mean_spikes)\n",
    "    data_csv.append(initial_std_spikes)\n",
    "    data_csv.append(attack_std_spikes)\n",
    "    data_csv.append(percent_shifts_initial_attack)\n",
    "\n",
    "    data_csv.append(a_initial_mean_spikes)\n",
    "    data_csv.append(a_attack_mean_spikes)\n",
    "    data_csv.append(a_initial_std_spikes)\n",
    "    data_csv.append(a_attack_std_spikes)\n",
    "    data_csv.append(a_mean_shifts)\n",
    "    \n",
    "    append_to_csv_file(stats_neurons_global_csv_filename, data_csv)\n",
    "    \n",
    "    #### 2) Statistics per each layer ####\n",
    "    for position in range(1, 4):\n",
    "        if position == 1:\n",
    "            min_range_time = T1_init\n",
    "            max_range_time = T1_fin\n",
    "        elif position == 2:\n",
    "            min_range_time = T2_init\n",
    "            max_range_time = T2_fin\n",
    "        elif position == 3:\n",
    "            min_range_time = T3_init\n",
    "            max_range_time = T3_fin\n",
    "        \n",
    "        data_csv = []\n",
    "        data_csv.append(test_id)\n",
    "        data_csv.append(position_attack)\n",
    "        data_csv.append(df_attack[\"n_attacks\"].unique()[0])\n",
    "        data_csv.append(df_attack[\"n_neurons\"].unique()[0])\n",
    "        data_csv.append(df_attack[\"stim_value\"].unique()[0])\n",
    "        data_csv.append(df_attack[\"vIncrement\"].unique()[0])\n",
    "        data_csv.append(df_attack[\"paramI\"].unique()[0])\n",
    "        data_csv.append(position)\n",
    "        \n",
    "        # We need to filter by time_delta before calculating statistics        \n",
    "        data_initial_state = df[(df.attack == \"initial_state\") & (df.time_delta >= min_range_time) & (df.time_delta < max_range_time)]\n",
    "        df_attack = df[(df.attack == attack) & (df.time_delta >= min_range_time) & (df.time_delta < max_range_time)]\n",
    "\n",
    "        list_attacked_neurons = df_attack[\"attacked_neurons\"].unique()[0].split(\"-\")\n",
    "\n",
    "        grouped = df_attack.groupby('neuron').count()[\"time_delta\"]\n",
    "        d_attack_aggr_neuron = pd.DataFrame()\n",
    "        d_attack_aggr_neuron['neuron'] = grouped.index\n",
    "        d_attack_aggr_neuron['n_spikes'] = grouped.tolist()\n",
    "\n",
    "        # Get the data from the initial_state experiment based on the paramI value\n",
    "        param_I = df_attack.paramI.unique()[0]\n",
    "\n",
    "        grouped = data_initial_state[(data_initial_state.paramI == param_I)].groupby('neuron').count()[\"time_delta\"]\n",
    "        d_initial_aggr_neuron = pd.DataFrame()\n",
    "        d_initial_aggr_neuron['neuron'] = grouped.index\n",
    "        d_initial_aggr_neuron['n_spikes'] = grouped.tolist()\n",
    "        \n",
    "        for layer in range(1, 4):\n",
    "            if layer == 1:\n",
    "                min_range_neuron = 0\n",
    "                max_range_neuron = 200\n",
    "            elif layer == 2:\n",
    "                min_range_neuron = 200\n",
    "                max_range_neuron = 272\n",
    "            elif layer == 3:\n",
    "                min_range_neuron = 272\n",
    "                max_range_neuron = 276\n",
    "\n",
    "            ### GLOBAL PERSPECTIVE\n",
    "            # Total number of spikes\n",
    "            initial_nSpikes = data_initial_state[(data_initial_state.paramI == param_I) & (data_initial_state.neuron >= min_range_neuron) & (data_initial_state.neuron < max_range_neuron)].count()[\"neuron\"]\n",
    "            attack_nSpikes = df_attack[(df_attack.neuron >= min_range_neuron) & (df_attack.neuron < max_range_neuron)].count()[\"neuron\"]\n",
    "\n",
    "            # Mean of spikes (over the total neurons of the layer)\n",
    "            initial_mean_spikes = round(d_initial_aggr_neuron[(d_initial_aggr_neuron.neuron >= min_range_neuron) & (d_initial_aggr_neuron.neuron < max_range_neuron)][\"n_spikes\"].mean(), 2)\n",
    "            attack_mean_spikes = round(d_attack_aggr_neuron[(d_attack_aggr_neuron.neuron >= min_range_neuron) & (d_attack_aggr_neuron.neuron < max_range_neuron)][\"n_spikes\"].mean(), 2)\n",
    "\n",
    "            # Standard deviation (over the total neurons of the layer)\n",
    "            initial_std_spikes = round(d_initial_aggr_neuron[(d_initial_aggr_neuron.neuron >= min_range_neuron) & (d_initial_aggr_neuron.neuron < max_range_neuron)][\"n_spikes\"].std(), 2)\n",
    "            attack_std_spikes = round(d_attack_aggr_neuron[(d_attack_aggr_neuron.neuron >= min_range_neuron) & (d_attack_aggr_neuron.neuron < max_range_neuron)][\"n_spikes\"].std(), 2)\n",
    "\n",
    "            # Percentage of shifts between the attack and normal behaviour\n",
    "            initial_nSpikes_join = data_initial_state[(data_initial_state.paramI == param_I) & (data_initial_state.neuron >= min_range_neuron) & (data_initial_state.neuron < max_range_neuron)][[\"attack\", \"time_delta\", \"neuron\"]].copy()\n",
    "            \n",
    "            attack_nSpikes_join = df_attack[(df_attack.neuron >= min_range_neuron) & (df_attack.neuron < max_range_neuron)][[\"attack\", \"time_delta\", \"neuron\"]].copy()\n",
    "            \n",
    "            join_initial_attack_shifts = pd.merge(attack_nSpikes_join, initial_nSpikes_join,  how='left', left_on=['time_delta','neuron'], right_on = ['time_delta','neuron'])\n",
    "            join_initial_attack_shifts = join_initial_attack_shifts.fillna(0)\n",
    "            n_shifts_initial_attack = join_initial_attack_shifts[join_initial_attack_shifts.attack_y == 0].count()[\"neuron\"]\n",
    "            percent_shifts_initial_attack = round((n_shifts_initial_attack/attack_nSpikes)*100,2)\n",
    "\n",
    "            # List of neurons shifted in this layer\n",
    "            attack_shifted_neurons = join_initial_attack_shifts[join_initial_attack_shifts.attack_y == 0][\"neuron\"].unique()\n",
    "\n",
    "            ### ATTACKED NEURONS PERSPECTIVE\n",
    "\n",
    "            # spikes: mean\n",
    "            a_initial_mean_spikes = round(d_initial_aggr_neuron[(d_initial_aggr_neuron.neuron.isin(attack_shifted_neurons))][\"n_spikes\"].mean(), 2)\n",
    "            a_attack_mean_spikes = round(d_attack_aggr_neuron[(d_attack_aggr_neuron.neuron.isin(attack_shifted_neurons))][\"n_spikes\"].mean(), 2)\n",
    "\n",
    "            # Standard deviation\n",
    "            a_initial_std_spikes = round(d_initial_aggr_neuron[(d_initial_aggr_neuron.neuron.isin(attack_shifted_neurons))][\"n_spikes\"].std(), 2)\n",
    "            a_attack_std_spikes = round(d_attack_aggr_neuron[(d_attack_aggr_neuron.neuron.isin(attack_shifted_neurons))][\"n_spikes\"].std(), 2)\n",
    "\n",
    "            # mean of % shifts\n",
    "            a_join_shifts_attk = join_initial_attack_shifts[join_initial_attack_shifts.neuron.isin(attack_shifted_neurons)]\n",
    "            a_n_shifts_zeros = a_join_shifts_attk[a_join_shifts_attk.attack_y == 0].count()[\"neuron\"]\n",
    "            # number of shifts under attack (count zeros) / total number of spikes under attack \n",
    "            a_mean_shifts = round((a_n_shifts_zeros/a_join_shifts_attk.neuron.count())*100, 2)\n",
    "            \n",
    "            data_csv.append(initial_mean_spikes)\n",
    "            data_csv.append(attack_mean_spikes)\n",
    "            data_csv.append(initial_std_spikes)\n",
    "            data_csv.append(attack_std_spikes)\n",
    "            data_csv.append(percent_shifts_initial_attack)\n",
    "\n",
    "            data_csv.append(a_initial_mean_spikes)\n",
    "            data_csv.append(a_attack_mean_spikes)\n",
    "            data_csv.append(a_initial_std_spikes)\n",
    "            data_csv.append(a_attack_std_spikes)\n",
    "            data_csv.append(a_mean_shifts)\n",
    "\n",
    "        append_to_csv_file(stats_neurons_layers_csv_filename, data_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_aggr_data_pandas(test_id, position_attack, attack, n_exec, vIncrement, n_neurons, flag, csv_filename, aggr_csv_filename):   \n",
    "    df = pd.read_csv(csv_filename, delimiter=\";\", dtype={'n_neurons': np.int32})\n",
    "\n",
    "    data_initial_state = df[df.attack == \"initial_state\"]\n",
    "    df_attack = df[(df.attack == attack) & (df.vIncrement == vIncrement)] \n",
    "\n",
    "    # Get the data from the initial_state experiment based on the paramI value\n",
    "    param_I = df_attack.paramI.unique()[0]\n",
    "    \n",
    "    for position in range(0, len(list_instant_optimal_path)):\n",
    "        min_range = list_instant_optimal_path[position]\n",
    "\n",
    "        if position == len(list_instant_optimal_path)-1: # last position of the list\n",
    "            max_range = round(SIMULATION_TIME/ms)\n",
    "        else:\n",
    "            max_range = list_instant_optimal_path[position+1]\n",
    "\n",
    "        # Dump spontaneous aggr data only if flag == True\n",
    "        if flag:\n",
    "            # Dump to CSV\n",
    "            # Group by time instant: Extraction of time_delta and count number of spikes in each instant\n",
    "            grouped = data_initial_state[(data_initial_state.paramI == param_I) & (data_initial_state.time_delta >= min_range) & (data_initial_state.time_delta < max_range)].groupby('time_delta').count()[\"neuron\"]\n",
    "            d_aggr_initial = pd.DataFrame()\n",
    "            d_aggr_initial['time_delta'] = grouped.index\n",
    "            d_aggr_initial['number_spikes'] = grouped.tolist() #[item/276 for item in grouped.tolist()]\n",
    "            d_aggr_initial[\"position_attack\"] = position_attack\n",
    "            \n",
    "            d_aggr_initial[\"attack\"] = \"initial_state\"\n",
    "            d_aggr_initial[\"position\"] = int(min_range/(STEP_TIME/ms)) #Get number of position\n",
    "            d_aggr_initial[\"n_neurons\"] = 0 # Initial state does not have n_neurons attacked\n",
    "            d_aggr_initial[\"v_increment\"] = 0 # Initial state does not have v_increment attack\n",
    "            d_aggr_initial[\"n_exec\"] = n_exec\n",
    "            \n",
    "            # Export both DF to the same CSV file (append)\n",
    "            d_aggr_initial.to_csv(aggr_csv_filename, mode = \"a\", index = False, header=False, sep=\";\")\n",
    "        \n",
    "        grouped = df_attack[(df_attack.paramI == param_I) & (df_attack.time_delta >= min_range) & (df_attack.time_delta < max_range)].groupby('time_delta').count()[\"neuron\"]\n",
    "        d_aggr_attack = pd.DataFrame()\n",
    "        d_aggr_attack['time_delta'] = grouped.index\n",
    "        d_aggr_attack['number_spikes'] = grouped.tolist() #[item/276 for item in grouped.tolist()]\n",
    "        d_aggr_attack[\"position_attack\"] = position_attack\n",
    "\n",
    "        d_aggr_attack[\"attack\"] = attack\n",
    "        d_aggr_attack[\"position\"] = int(min_range/(STEP_TIME/ms)) #Get number of position\n",
    "        d_aggr_attack[\"n_neurons\"] = n_neurons\n",
    "        d_aggr_attack[\"v_increment\"] = vIncrement\n",
    "        d_aggr_attack[\"n_exec\"] = n_exec\n",
    "\n",
    "        d_aggr_attack.to_csv(aggr_csv_filename, mode = \"a\", index = False, header=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"FLO\" in list_attack_generation:\n",
    "    currTest = 1\n",
    "    stimValue = 1.0\n",
    "    paramI = 10\n",
    "    #currExec = 0\n",
    "    nNeurons = 100\n",
    "    position_attack = 0\n",
    "    param_I = 0\n",
    "\n",
    "    spontaneous_params = {\n",
    "        \"attack\": \"initial_state\"\n",
    "    }\n",
    "\n",
    "    spikeMon, stateMon = neuronal_simulation(SIMULATION_TIME, spontaneous_params, {})\n",
    "\n",
    "    flag_initial = True\n",
    "    for currExec in range(0, 10):\n",
    "        print(currExec)\n",
    "\n",
    "        FLO_params = {\n",
    "            \"attack\": \"FLO\",\n",
    "            \"instants_attack\": [10*ms],\n",
    "            \"neuron_list\": generate_list_random_neurons(nNeurons),\n",
    "            \"vIncrement\": 40,\n",
    "        }\n",
    "\n",
    "        FLO_results_simulation = {}\n",
    "\n",
    "        spikeMon_a, stateMon_a = neuronal_simulation(SIMULATION_TIME, FLO_params, FLO_results_simulation)\n",
    "\n",
    "        dump_simulation_data_to_csv(\"initial_state\", str(currTest), str(position_attack), \"0\", \"0\", \"X\", \"X\", \"0\", \"0\", \"0\", str(paramI), spikeMon, csv_filename_FLO) \n",
    "        dump_simulation_data_to_csv(\"FLO\", str(currTest), str(position_attack), str(len(FLO_params[\"instants_attack\"])), str(len(FLO_params[\"neuron_list\"])), list_neurons_to_string(FLO_params[\"neuron_list\"]), \"(0,0)\", str(stimValue), str(currExec), str(FLO_params[\"vIncrement\"]), str(paramI), spikeMon_a, csv_filename_FLO)\n",
    "\n",
    "        generate_aggr_data_pandas(currTest, position_attack, \"FLO\", currExec, FLO_params[\"vIncrement\"], nNeurons, flag_initial, csv_filename_FLO, aggr_csv_filename_FLO)\n",
    "\n",
    "        if flag_initial:\n",
    "            flag_initial = False\n",
    "\n",
    "        generate_stats_neurons_pandas(currTest, position_attack, \"FLO\", csv_filename_FLO, stats_neurons_global_csv_filename_FLO, stats_neurons_layers_csv_filename_FLO)    \n",
    "\n",
    "        # Overwrite export file for next execution\n",
    "        open(csv_filename_FLO, 'w').close()\n",
    "        append_to_csv_file(csv_filename_FLO, [\"attack\", \"test\", \"position_attack\", \"n_attacks\", \"n_neurons\", \"attacked_neurons\", \"coord_attack\", \"stim_value\", \"n_exec\", \"vIncrement\", \"paramI\", \"time_delta\", \"neuron\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_spikes_FLO = pd.read_csv(aggr_csv_filename_FLO, delimiter=\";\")\n",
    "df_total_spikes_FLO = df_total_spikes_FLO.drop(['position_attack'], axis = 1)\n",
    "df_total_spikes_FLO = pd.DataFrame(df_total_spikes_FLO.groupby([\"attack\", \"position\", \"n_exec\"])[\"number_spikes\"].sum())\n",
    "df_total_spikes_FLO.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_spikes_FLO.to_csv(stats_dir_FLO+'total_spikes.csv', index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"JAM\" in list_attack_generation:\n",
    "    currTest = 1\n",
    "    stimValue = 1.0\n",
    "    vIncrement = 0\n",
    "    paramI = 10\n",
    "    #currExec = 0\n",
    "    nNeurons = 100\n",
    "    position_attack = 0\n",
    "\n",
    "    spontaneous_params = {\n",
    "        \"attack\": \"initial_state\"\n",
    "    }\n",
    "\n",
    "    spikeMon, stateMon = neuronal_simulation(SIMULATION_TIME, spontaneous_params, {})\n",
    "\n",
    "    flag_initial = True\n",
    "    for currExec in range(0, 10):\n",
    "        print(currExec)\n",
    "\n",
    "        JAM_params = {\n",
    "            \"attack\": \"JAM\",\n",
    "            \"init_attack\": 10*ms,\n",
    "            \"end_attack\": 60*ms,\n",
    "            \"neuron_list\": generate_list_random_neurons(nNeurons),\n",
    "        }\n",
    "\n",
    "        JAM_results_simulation = {}\n",
    "\n",
    "        spikeMon_a, stateMon_a = neuronal_simulation(SIMULATION_TIME, JAM_params, JAM_results_simulation)\n",
    "\n",
    "        dump_simulation_data_to_csv(\"initial_state\", str(currTest), str(position_attack), \"0\", \"0\", \"X\", \"X\", \"0\", \"0\", \"0\", str(paramI), spikeMon, csv_filename_JAM) \n",
    "        dump_simulation_data_to_csv(\"JAM\", str(currTest), str(position_attack), \"1\", str(len(JAM_params[\"neuron_list\"])), list_neurons_to_string(JAM_params[\"neuron_list\"]), \"(0,0)\", str(stimValue), str(currExec), str(vIncrement), str(paramI), spikeMon_a, csv_filename_JAM)\n",
    "\n",
    "        generate_aggr_data_pandas(currTest, position_attack, \"JAM\", currExec, vIncrement, nNeurons, flag_initial, csv_filename_JAM, aggr_csv_filename_JAM)\n",
    "\n",
    "        if flag_initial:\n",
    "            flag_initial = False\n",
    "\n",
    "        generate_stats_neurons_pandas(currTest, position_attack, \"JAM\", csv_filename_JAM, stats_neurons_global_csv_filename_JAM, stats_neurons_layers_csv_filename_JAM)    \n",
    "\n",
    "        # Overwrite export file for next execution\n",
    "        open(csv_filename_JAM, 'w').close()\n",
    "        append_to_csv_file(csv_filename_JAM, [\"attack\", \"test\", \"position_attack\", \"n_attacks\", \"n_neurons\", \"attacked_neurons\", \"coord_attack\", \"stim_value\", \"n_exec\", \"vIncrement\", \"paramI\", \"time_delta\", \"neuron\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_spikes_JAM = pd.read_csv(aggr_csv_filename_JAM, delimiter=\";\")\n",
    "df_total_spikes_JAM = df_total_spikes_JAM.drop(['position_attack'], axis = 1)\n",
    "df_total_spikes_JAM = pd.DataFrame(df_total_spikes_JAM.groupby([\"attack\", \"position\", \"n_exec\"])[\"number_spikes\"].sum())\n",
    "df_total_spikes_JAM.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_spikes_JAM.to_csv(stats_dir_JAM+'total_spikes.csv', index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"SCA\" in list_attack_generation:\n",
    "    currTest = 1\n",
    "    stimValue = 1.0\n",
    "    paramI = 10\n",
    "    currExec = 0\n",
    "    nNeurons = 200\n",
    "    position_attack = 0\n",
    "\n",
    "    spontaneous_params = {\n",
    "        \"attack\": \"initial_state\"\n",
    "    }\n",
    "\n",
    "    spikeMon, stateMon = neuronal_simulation(SIMULATION_TIME, spontaneous_params, {})\n",
    "\n",
    "    SCA_params = {\n",
    "        \"attack\": \"SCA\",\n",
    "        \"init_attack\": 10*ms,\n",
    "        \"steps_attack\": get_time_steps_sequential(10*ms),\n",
    "        \"vIncrement\": 40,\n",
    "    }\n",
    "\n",
    "    SCA_results_simulation = {}\n",
    "\n",
    "    spikeMon_a, stateMon_a = neuronal_simulation(SIMULATION_TIME, SCA_params, SCA_results_simulation)\n",
    "\n",
    "    dump_simulation_data_to_csv(\"initial_state\", str(currTest), str(position_attack), \"0\", \"0\", \"X\", \"X\", \"0\", \"0\", \"0\", str(paramI), spikeMon, csv_filename_SCA) \n",
    "    dump_simulation_data_to_csv(\"SCA\", str(currTest), str(position_attack), \"1\", str(200), \"X\", \"(0,0)\", str(stimValue), str(currExec), str(SCA_params[\"vIncrement\"]), str(paramI), spikeMon_a, csv_filename_SCA)\n",
    "\n",
    "    generate_aggr_data_pandas(currTest, position_attack, \"SCA\", currExec, SCA_params[\"vIncrement\"], nNeurons, True, csv_filename_SCA, aggr_csv_filename_SCA)\n",
    "    generate_stats_neurons_pandas(currTest, position_attack, \"SCA\", csv_filename_SCA, stats_neurons_global_csv_filename_SCA, stats_neurons_layers_csv_filename_SCA)    \n",
    "\n",
    "    # Overwrite export file for next execution\n",
    "    open(csv_filename_SCA, 'w').close()\n",
    "    append_to_csv_file(csv_filename_SCA, [\"attack\", \"test\", \"position_attack\", \"n_attacks\", \"n_neurons\", \"attacked_neurons\", \"coord_attack\", \"stim_value\", \"n_exec\", \"vIncrement\", \"paramI\", \"time_delta\", \"neuron\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_total_spikes_SCA = pd.read_csv(aggr_csv_filename_SCA, delimiter=\";\")\n",
    "df_total_spikes_SCA = df_total_spikes_SCA.drop(['position_attack', 'n_exec'], axis = 1)\n",
    "df_total_spikes_SCA = pd.DataFrame(df_total_spikes_SCA.groupby([\"attack\", \"position\"])[\"number_spikes\"].sum())\n",
    "df_total_spikes_SCA.reset_index(inplace=True)\n",
    "df_total_spikes_SCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_spikes_SCA.to_csv(stats_dir_SCA+'total_spikes.csv', index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if \"FOR\" in list_attack_generation:\n",
    "    currTest = 1\n",
    "    stimValue = 1.0\n",
    "    paramI = 10\n",
    "    currExec = 0\n",
    "    nNeurons = 200\n",
    "    position_attack = 0\n",
    "\n",
    "    spontaneous_params = {\n",
    "        \"attack\": \"initial_state\"\n",
    "    }\n",
    "\n",
    "    spikeMon, stateMon = neuronal_simulation(SIMULATION_TIME, spontaneous_params, {})\n",
    "\n",
    "    FOR_params = {\n",
    "        \"attack\": \"FOR\",\n",
    "        \"init_attack\": 10*ms,\n",
    "        \"steps_attack\": get_time_steps_sequential(10*ms),\n",
    "        \"vIncrement\": 40,\n",
    "    }\n",
    "\n",
    "    FOR_results_simulation = {}\n",
    "\n",
    "    spikeMon_a, stateMon_a = neuronal_simulation(SIMULATION_TIME, FOR_params, FOR_results_simulation)\n",
    "\n",
    "    dump_simulation_data_to_csv(\"initial_state\", str(currTest), str(position_attack), \"0\", \"0\", \"X\", \"X\", \"0\", \"0\", \"0\", str(paramI), spikeMon, csv_filename_FOR) \n",
    "    dump_simulation_data_to_csv(\"FOR\", str(currTest), str(position_attack), \"1\", str(200), \"X\", \"(0,0)\", str(stimValue), str(currExec), str(FOR_params[\"vIncrement\"]), str(paramI), spikeMon_a, csv_filename_FOR)\n",
    "\n",
    "    generate_aggr_data_pandas(currTest, position_attack, \"FOR\", currExec, FOR_params[\"vIncrement\"], nNeurons, True, csv_filename_FOR, aggr_csv_filename_FOR)\n",
    "    generate_stats_neurons_pandas(currTest, position_attack, \"FOR\", csv_filename_FOR, stats_neurons_global_csv_filename_FOR, stats_neurons_layers_csv_filename_FOR)    \n",
    "\n",
    "    # Overwrite export file for next execution\n",
    "    open(csv_filename_FOR, 'w').close()\n",
    "    append_to_csv_file(csv_filename_FOR, [\"attack\", \"test\", \"position_attack\", \"n_attacks\", \"n_neurons\", \"attacked_neurons\", \"coord_attack\", \"stim_value\", \"n_exec\", \"vIncrement\", \"paramI\", \"time_delta\", \"neuron\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_total_spikes_FOR = pd.read_csv(aggr_csv_filename_FOR, delimiter=\";\")\n",
    "df_total_spikes_FOR = df_total_spikes_FOR.drop(['position_attack', 'n_exec'], axis = 1)\n",
    "df_total_spikes_FOR = pd.DataFrame(df_total_spikes_FOR.groupby([\"attack\", \"position\"])[\"number_spikes\"].sum())\n",
    "df_total_spikes_FOR.reset_index(inplace=True)\n",
    "df_total_spikes_FOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_spikes_FOR.to_csv(stats_dir_FOR+'total_spikes.csv', index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"SPO\" in list_attack_generation:\n",
    "    currTest = 1\n",
    "    stimValue = 1.0\n",
    "    vIncrement = 0\n",
    "    paramI = 10\n",
    "    currExec = 0\n",
    "    nNeurons = 200\n",
    "    position_attack = 0\n",
    "\n",
    "    spontaneous_params = {\n",
    "        \"attack\": \"initial_state\"\n",
    "    }\n",
    "\n",
    "    spikeMon, stateMon = neuronal_simulation(SIMULATION_TIME, spontaneous_params, {})\n",
    "\n",
    "    SPO_params = {\n",
    "        \"attack\": \"SPO\",\n",
    "        \"neuron_list_recorded\": list(range(0, 100,1)),\n",
    "        \"neuron_list_attack\": list(range(100,200,1)),\n",
    "        \"init_record\": 10*ms,\n",
    "        \"end_record\": 60*ms,\n",
    "        \"init_attack\": 70*ms,\n",
    "        \"end_attack\": 120*ms,\n",
    "        \"recorded_activity\": {}\n",
    "    }\n",
    "\n",
    "    # Extract legitimate neuronal activity (record) to be replicated by the SPO attack\n",
    "    init_record_formatted = round(SPO_params[\"init_record\"]/ms*10)\n",
    "    end_record_formatted = round(SPO_params[\"end_record\"]/ms*10)\n",
    "\n",
    "    # Extend SPO_params with the recorded activity from the spontaneous behavior for the targeted neurons\n",
    "    neuron_counter = 0\n",
    "    for neuron in SPO_params[\"neuron_list_recorded\"]:\n",
    "        SPO_params[\"recorded_activity\"][SPO_params[\"neuron_list_attack\"][neuron_counter]] = list(stateMon.v[neuron][init_record_formatted: end_record_formatted])\n",
    "        neuron_counter += 1\n",
    "\n",
    "    spikeMon_a, stateMon_a = neuronal_simulation(SIMULATION_TIME, SPO_params, {})\n",
    "    \n",
    "    dump_simulation_data_to_csv(\"initial_state\", str(currTest), str(position_attack), \"0\", \"0\", \"X\", \"X\", \"0\", \"0\", \"0\", str(paramI), spikeMon, csv_filename_SPO) \n",
    "    dump_simulation_data_to_csv(\"SPO\", str(currTest), str(position_attack), \"1\", str(200), \"X\", \"(0,0)\", str(stimValue), str(currExec), str(vIncrement), str(paramI), spikeMon_a, csv_filename_SPO)\n",
    "\n",
    "    generate_aggr_data_pandas(currTest, position_attack, \"SPO\", currExec, vIncrement, nNeurons, True, csv_filename_SPO, aggr_csv_filename_SPO)\n",
    "    generate_stats_neurons_pandas(currTest, position_attack, \"SPO\", csv_filename_SPO, stats_neurons_global_csv_filename_SPO, stats_neurons_layers_csv_filename_SPO)    \n",
    "\n",
    "    # Overwrite export file for next execution\n",
    "    open(csv_filename_SPO, 'w').close()\n",
    "    append_to_csv_file(csv_filename_SPO, [\"attack\", \"test\", \"position_attack\", \"n_attacks\", \"n_neurons\", \"attacked_neurons\", \"coord_attack\", \"stim_value\", \"n_exec\", \"vIncrement\", \"paramI\", \"time_delta\", \"neuron\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_total_spikes_SPO = pd.read_csv(aggr_csv_filename_SPO, delimiter=\";\")\n",
    "df_total_spikes_SPO = df_total_spikes_SPO.drop(['position_attack', 'n_exec'], axis = 1)\n",
    "df_total_spikes_SPO = pd.DataFrame(df_total_spikes_SPO.groupby([\"attack\", \"position\"])[\"number_spikes\"].sum())\n",
    "df_total_spikes_SPO.reset_index(inplace=True)\n",
    "df_total_spikes_SPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_spikes_SPO.to_csv(stats_dir_SPO+'total_spikes.csv', index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_total_spikes_SPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SYB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"SYB\" in list_attack_generation:\n",
    "    currTest = 1\n",
    "    stimValue = 1.0\n",
    "    vIncrement = 0\n",
    "    paramI = 10\n",
    "    #currExec = 0\n",
    "    nNeurons = 100\n",
    "    position_attack = 0\n",
    "    param_I = 0\n",
    "\n",
    "    spontaneous_params = {\n",
    "        \"attack\": \"initial_state\"\n",
    "    }\n",
    "\n",
    "    spikeMon, stateMon = neuronal_simulation(SIMULATION_TIME, spontaneous_params, {})\n",
    "\n",
    "    flag_initial = True\n",
    "\n",
    "    for currExec in range(0, 10):\n",
    "        print(currExec)\n",
    "        SYB_params = {\n",
    "            \"attack\": \"SYB\",\n",
    "            \"instants_attack\": [10*ms],\n",
    "            \"neuron_list\": generate_list_random_neurons(nNeurons),\n",
    "        }\n",
    "\n",
    "        SYB_results_simulation = {\n",
    "            \"stimulation\": {}, \n",
    "            \"inhibition\": {},\n",
    "        }\n",
    "\n",
    "        spikeMon_a, stateMon_a = neuronal_simulation(SIMULATION_TIME, SYB_params, SYB_results_simulation)\n",
    "\n",
    "        dump_simulation_data_to_csv(\"initial_state\", str(currTest), str(position_attack), \"0\", \"0\", \"X\", \"X\", \"0\", \"0\", \"0\", str(paramI), spikeMon, csv_filename_SYB) \n",
    "        dump_simulation_data_to_csv(\"SYB\", str(currTest), str(position_attack), str(len(SYB_params[\"instants_attack\"])), str(len(SYB_params[\"neuron_list\"])), list_neurons_to_string(SYB_params[\"neuron_list\"]), \"(0,0)\", str(stimValue), str(currExec), str(vIncrement), str(paramI), spikeMon_a, csv_filename_SYB)\n",
    "\n",
    "        generate_aggr_data_pandas(currTest, position_attack, \"SYB\", currExec, vIncrement, nNeurons, flag_initial, csv_filename_SYB, aggr_csv_filename_SYB)\n",
    "\n",
    "        if flag_initial:\n",
    "            flag_initial = False\n",
    "\n",
    "        generate_stats_neurons_pandas(currTest, position_attack, \"SYB\", csv_filename_SYB, stats_neurons_global_csv_filename_SYB, stats_neurons_layers_csv_filename_SYB)    \n",
    "\n",
    "        # Overwrite export file for next execution\n",
    "        open(csv_filename_SYB, 'w').close()\n",
    "        append_to_csv_file(csv_filename_SYB, [\"attack\", \"test\", \"position_attack\", \"n_attacks\", \"n_neurons\", \"attacked_neurons\", \"coord_attack\", \"stim_value\", \"n_exec\", \"vIncrement\", \"paramI\", \"time_delta\", \"neuron\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_spikes_SYB = pd.read_csv(aggr_csv_filename_SYB, delimiter=\";\")\n",
    "df_total_spikes_SYB = df_total_spikes_SYB.drop(['position_attack'], axis = 1)\n",
    "df_total_spikes_SYB = pd.DataFrame(df_total_spikes_SYB.groupby([\"attack\", \"position\", \"n_exec\"])[\"number_spikes\"].sum())\n",
    "df_total_spikes_SYB.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_spikes_SYB.to_csv(stats_dir_SYB+'total_spikes.csv', index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_spikes_SYB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"SIN\" in list_attack_generation:\n",
    "    currTest = 1\n",
    "    stimValue = 1.0\n",
    "    paramI = 10\n",
    "    currExec = 0\n",
    "    nNeurons = 100\n",
    "    position_attack = 0\n",
    "    param_I = 0\n",
    "\n",
    "    spontaneous_params = {\n",
    "        \"attack\": \"initial_state\"\n",
    "    }\n",
    "\n",
    "    spikeMon, stateMon = neuronal_simulation(SIMULATION_TIME, spontaneous_params, {})\n",
    "\n",
    "    SIN_params = {\n",
    "        \"attack\": \"SIN\",\n",
    "        \"instants_attack\": [10*ms],\n",
    "        \"vIncrement\": 40,\n",
    "        \"target_neuron_2nd_layer\": 0,\n",
    "    }\n",
    "\n",
    "    SIN_results_simulation = {}\n",
    "\n",
    "    # Obtain the list of neurons connected to the target neuron from the 2nd layer -> attacked neurons\n",
    "    df_neurons = pd.read_csv(\"synapsysConv1-Conv2.csv\", delimiter=\";\", usecols=[\"sourceNeuron\", \"targetNeuron\"])\n",
    "    SIN_params[\"neuron_list\"] = df_neurons[df_neurons.targetNeuron == SIN_params[\"target_neuron_2nd_layer\"]].sourceNeuron.unique().tolist()\n",
    "\n",
    "    spikeMon_a, stateMon_a = neuronal_simulation(SIMULATION_TIME, SIN_params, SIN_results_simulation)\n",
    "\n",
    "    dump_simulation_data_to_csv(\"initial_state\", str(currTest), str(position_attack), \"0\", \"0\", \"X\", \"X\", \"0\", \"0\", \"0\", str(paramI), spikeMon, csv_filename_SIN) \n",
    "    dump_simulation_data_to_csv(\"SIN\", str(currTest), str(position_attack), str(1), str(len(SIN_params[\"neuron_list\"])), list_neurons_to_string(SIN_params[\"neuron_list\"]), \"(0,0)\", str(stimValue), str(currExec), str(SIN_params[\"vIncrement\"]), str(paramI), spikeMon_a, csv_filename_SIN)\n",
    "\n",
    "    generate_aggr_data_pandas(currTest, position_attack, \"SIN\", currExec, SIN_params[\"vIncrement\"], nNeurons, True, csv_filename_SIN, aggr_csv_filename_SIN)\n",
    "    generate_stats_neurons_pandas(currTest, position_attack, \"SIN\", csv_filename_SIN, stats_neurons_global_csv_filename_SIN, stats_neurons_layers_csv_filename_SIN)    \n",
    "\n",
    "    # Overwrite export file for next execution\n",
    "    open(csv_filename_SIN, 'w').close()\n",
    "    append_to_csv_file(csv_filename_SIN, [\"attack\", \"test\", \"position_attack\", \"n_attacks\", \"n_neurons\", \"attacked_neurons\", \"coord_attack\", \"stim_value\", \"n_exec\", \"vIncrement\", \"paramI\", \"time_delta\", \"neuron\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_spikes_SIN = pd.read_csv(aggr_csv_filename_SIN, delimiter=\";\")\n",
    "df_total_spikes_SIN = df_total_spikes_SIN.drop(['position_attack', 'n_exec'], axis = 1)\n",
    "df_total_spikes_SIN = pd.DataFrame(df_total_spikes_SIN.groupby([\"attack\", \"position\"])[\"number_spikes\"].sum())\n",
    "df_total_spikes_SIN.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_spikes_SIN.to_csv(stats_dir_SIN+'total_spikes.csv', index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"NON\" in list_attack_generation:\n",
    "    currTest = 1\n",
    "    stimValue = 1.0\n",
    "    currExec = 0\n",
    "    nNeurons = 105\n",
    "    vIncrement = 0\n",
    "    position_attack = 0\n",
    "    param_I = 0\n",
    "\n",
    "    spontaneous_params = {\n",
    "        \"attack\": \"initial_state\"\n",
    "    }\n",
    "\n",
    "    spikeMon, stateMon = neuronal_simulation(SIMULATION_TIME, spontaneous_params, {})\n",
    "\n",
    "    NON_params = {\n",
    "        \"attack\": \"NON\",\n",
    "        \"init_attack\": 10*ms,\n",
    "        \"steps_attack\": get_time_steps_sequential(10*ms),\n",
    "        \"neuron_list\": list(range(0,200,1)), # All neurons in the first layer\n",
    "        \"vIncrement\": 40,\n",
    "        \"probability_spontaneous\": 60,\n",
    "        \"probability_stimulation\": 20,\n",
    "        \"probability_inhibition\": 20,\n",
    "    }\n",
    "\n",
    "    NON_results_simulation = {\n",
    "        \"stimulation\": {}, \n",
    "        \"inhibition\": {},\n",
    "    }\n",
    "\n",
    "    spikeMon_a, stateMon_a = neuronal_simulation(SIMULATION_TIME, NON_params, NON_results_simulation)\n",
    "\n",
    "    dump_simulation_data_to_csv(\"initial_state\", str(currTest), str(position_attack), \"0\", \"0\", \"X\", \"X\", \"0\", \"0\", \"0\", str(paramI), spikeMon, csv_filename_NON) \n",
    "    dump_simulation_data_to_csv(\"NON\", str(currTest), str(position_attack), str(1), str(len(NON_params[\"neuron_list\"])), list_neurons_to_string(NON_params[\"neuron_list\"]), \"(0,0)\", str(stimValue), str(currExec), str(NON_params[\"vIncrement\"]), str(paramI), spikeMon_a, csv_filename_NON)\n",
    "\n",
    "    generate_aggr_data_pandas(currTest, position_attack, \"NON\", currExec, NON_params[\"vIncrement\"], nNeurons, True, csv_filename_NON, aggr_csv_filename_NON)\n",
    "    generate_stats_neurons_pandas(currTest, position_attack, \"NON\", csv_filename_NON, stats_neurons_global_csv_filename_NON, stats_neurons_layers_csv_filename_NON)    \n",
    "\n",
    "    # Overwrite export file for next execution\n",
    "    open(csv_filename_NON, 'w').close()\n",
    "    append_to_csv_file(csv_filename_NON, [\"attack\", \"test\", \"position_attack\", \"n_attacks\", \"n_neurons\", \"attacked_neurons\", \"coord_attack\", \"stim_value\", \"n_exec\", \"vIncrement\", \"paramI\", \"time_delta\", \"neuron\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_spikes_NON = pd.read_csv(aggr_csv_filename_NON, delimiter=\";\")\n",
    "df_total_spikes_NON = df_total_spikes_NON.drop(['position_attack', 'n_exec'], axis = 1)\n",
    "df_total_spikes_NON = pd.DataFrame(df_total_spikes_NON.groupby([\"attack\", \"position\"])[\"number_spikes\"].sum())\n",
    "df_total_spikes_NON.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_spikes_NON.to_csv(stats_dir_NON+'total_spikes.csv', index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate plot for results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.family': 'monospace'})\n",
    "sns.set(style=\"whitegrid\",font_scale=2.5, rc={'figure.figsize':(40,20)})\n",
    "\n",
    "#kwargs  =   {'edgecolor':\"black\", # for edge color\n",
    "#             'linewidth':2, # line width of spot\n",
    "#             'linestyle':'-', # line style of spot\n",
    "#            }\n",
    "\n",
    "plt.rcParams['axes.edgecolor'] = \"black\" #set the value globally\n",
    "sns.set_palette(\"colorblind\")\n",
    "#sns.set(font_scale=1.4)\n",
    "#sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FLO = pd.read_csv(stats_dir_FLO+'total_spikes.csv', delimiter=\";\")\n",
    "df_JAM = pd.read_csv(stats_dir_JAM+'total_spikes.csv', delimiter=\";\")\n",
    "df_SCA = pd.read_csv(stats_dir_SCA+'total_spikes.csv', delimiter=\";\")\n",
    "df_FOR = pd.read_csv(stats_dir_FOR+'total_spikes.csv', delimiter=\";\")\n",
    "df_SPO = pd.read_csv(stats_dir_SPO+'total_spikes.csv', delimiter=\";\")\n",
    "df_SYB = pd.read_csv(stats_dir_SYB+'total_spikes.csv', delimiter=\";\")\n",
    "df_SIN = pd.read_csv(stats_dir_SIN+'total_spikes.csv', delimiter=\";\")\n",
    "df_NON = pd.read_csv(stats_dir_NON+'total_spikes.csv', delimiter=\";\")\n",
    "\n",
    "df_SCA[\"n_exec\"] = 0\n",
    "df_FOR[\"n_exec\"] = 0\n",
    "df_SPO[\"n_exec\"] = 0\n",
    "df_SIN[\"n_exec\"] = 0\n",
    "df_NON[\"n_exec\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spikes = pd.concat([df_FLO, df_JAM[df_JAM.attack == \"JAM\"], \n",
    "                    df_SCA[df_SCA.attack == \"SCA\"], df_FOR[df_FOR.attack == \"FOR\"], \n",
    "                    df_SPO[df_SPO.attack == \"SPO\"], df_SYB[df_SYB.attack == \"SYB\"], \n",
    "                    df_SIN[df_SIN.attack == \"SIN\"], df_NON[df_NON.attack == \"NON\"]])\n",
    "\n",
    "# Rename \"initial_state\" to \"Spontaneous\"\n",
    "df_spikes.loc[df_spikes['attack']==\"initial_state\",'attack'] = 'Spontaneous'\n",
    "df_spikes.position += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DF CONTAINING PERCENT CHANGES FOR EACH ATTACK, POSITION AND N_EXEC, READY TO PLOT ##\n",
    "\n",
    "df_percentages = pd.DataFrame()\n",
    "\n",
    "# (v2 - v2)/|v1| x 100\n",
    "\n",
    "# For those attacks with multiple executions, we calculate the percentage change for each execution.\n",
    "for attack in list_attack_generation:   \n",
    "    for position in range(1, 6): \n",
    "        v1 = df_spikes[(df_spikes.attack == \"Spontaneous\") & (df_spikes.position == position)].mean()[\"number_spikes\"]\n",
    "            \n",
    "        if attack in [\"FLO\", \"JAM\", \"SYB\"]:\n",
    "            v2_list = df_spikes[(df_spikes.attack == attack) & (df_spikes.position == position)][\"number_spikes\"]\n",
    "            \n",
    "            counter = 1\n",
    "            for v2_value in v2_list:\n",
    "                percent_change = (v2_value-v1)/abs(v1) * 100\n",
    "                \n",
    "                data = {\n",
    "                    \"attack\": [attack],\n",
    "                    \"position\": [position],\n",
    "                    \"exec\": [counter],\n",
    "                    \"percentage\": [round(percent_change, 2)]\n",
    "                }\n",
    "\n",
    "                df_row = pd.DataFrame(data)\n",
    "                df_percentages = pd.concat([df_percentages, df_row])\n",
    "                \n",
    "                counter += 1\n",
    "                \n",
    "        else:\n",
    "            v2 = df_spikes[(df_spikes.attack == attack) & (df_spikes.position == position)].mean()[\"number_spikes\"]\n",
    "\n",
    "            percent_change = (v2-v1)/abs(v1) * 100\n",
    "            \n",
    "            data = {\n",
    "                \"attack\": [attack],\n",
    "                \"position\": [position],\n",
    "                \"exec\": [counter],\n",
    "                \"percentage\": [round(percent_change, 2)]\n",
    "            }\n",
    "\n",
    "            df_row = pd.DataFrame(data)\n",
    "            df_percentages = pd.concat([df_percentages, df_row])\n",
    "    \n",
    "    for position in range(23, 28): \n",
    "        v1 = df_spikes[(df_spikes.attack == \"Spontaneous\") & (df_spikes.position == position)].mean()[\"number_spikes\"]\n",
    "            \n",
    "        if attack in [\"FLO\", \"JAM\", \"SYB\"]:\n",
    "            v2_list = df_spikes[(df_spikes.attack == attack) & (df_spikes.position == position)][\"number_spikes\"]\n",
    "            \n",
    "            counter = 1\n",
    "            for v2_value in v2_list:\n",
    "                percent_change = (v2_value-v1)/abs(v1) * 100\n",
    "                #percentages[attack+\"_final\"].append(round(percent_change, 2))\n",
    "                \n",
    "                data = {\n",
    "                    \"attack\": [attack],\n",
    "                    \"position\": [position],\n",
    "                    \"exec\": [counter],\n",
    "                    \"percentage\": [round(percent_change, 2)]\n",
    "                }\n",
    "\n",
    "                df_row = pd.DataFrame(data)\n",
    "                df_percentages = pd.concat([df_percentages, df_row])\n",
    "                \n",
    "                counter += 1\n",
    "        else:\n",
    "            v2 = df_spikes[(df_spikes.attack == attack) & (df_spikes.position == position)].mean()[\"number_spikes\"]\n",
    "\n",
    "            percent_change = (v2-v1)/abs(v1) * 100\n",
    "            \n",
    "            data = {\n",
    "                \"attack\": [attack],\n",
    "                \"position\": [position],\n",
    "                \"exec\": [counter],\n",
    "                \"percentage\": [round(percent_change, 2)]\n",
    "            }\n",
    "\n",
    "            df_row = pd.DataFrame(data)\n",
    "            df_percentages = pd.concat([df_percentages, df_row])\n",
    "            \n",
    "# Multiply by -1 to obtain the decrease % in a positive value (instead of -x%)    \n",
    "df_percentages[\"percentage\"] *= -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new column to indicate the group of positions\n",
    "df_first_group = df_percentages[df_percentages.position.isin([1,2,3,4,5])].copy()\n",
    "df_first_group[\"Window\"] = \"First five positions\"\n",
    "\n",
    "df_spikes_plot = df_first_group.copy()\n",
    "\n",
    "df_last_group = df_percentages[df_percentages.position.isin([23,24,25,26,27])].copy()\n",
    "df_last_group[\"Window\"] = \"Last five positions\"\n",
    "\n",
    "df_spikes_plot = pd.concat([df_spikes_plot, df_last_group])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(figsize=(40,20))\n",
    "\n",
    "g = sns.catplot(x=\"attack\", y=\"percentage\", col=\"Window\", data=df_spikes_plot, kind=\"bar\", height=10, aspect=2)\n",
    "\n",
    "label_size = 45\n",
    "ticks_size = 35\n",
    "legend_size = 35\n",
    "numbers_size = 35\n",
    "\n",
    "ax = g.facet_axis(0,0)\n",
    "ax.set_xlabel(\"Attack\", fontsize=label_size, fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Decrease percentage\", fontsize=label_size, fontweight=\"bold\")\n",
    "ax.tick_params(labelsize=ticks_size)\n",
    "\n",
    "ax.grid(b=True, which='major', color='#d8dcd6', linewidth=1.0)\n",
    "ax.grid(b=True, which='minor', color='#d8dcd6', linewidth=0.5)\n",
    "\n",
    "ax = g.facet_axis(0,1)\n",
    "ax.set_xlabel(\"Attack\", fontsize=label_size, fontweight=\"bold\")\n",
    "ax.tick_params(labelsize=ticks_size)\n",
    "\n",
    "ax.grid(b=True, which='major', color='#d8dcd6', linewidth=1.0)\n",
    "ax.grid(b=True, which='minor', color='#d8dcd6', linewidth=0.5)\n",
    "\n",
    "ax = g.facet_axis(0,0)\n",
    "\n",
    "counter = 1\n",
    "\n",
    "distances_initial = {\n",
    "    1: 50,\n",
    "    2: 45, \n",
    "    3: 60, \n",
    "    4: 45,\n",
    "    5: 90, \n",
    "    6: 50, \n",
    "    7: 60,\n",
    "    8: 135, \n",
    "}\n",
    "\n",
    "\n",
    "counter = 1\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height(), '.2f'), \n",
    "                    (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                    ha = 'center', va = 'center', \n",
    "                    size=numbers_size,\n",
    "                    xytext = (0, distances_initial[counter]), \n",
    "                    textcoords = 'offset points')\n",
    "    counter+=1\n",
    "\n",
    "ax = g.facet_axis(0,1)\n",
    "\n",
    "distances_final = {\n",
    "    1: 40,\n",
    "    2: 40, \n",
    "    3: 50, \n",
    "    4: 30,\n",
    "    5: 70, \n",
    "    6: 40, \n",
    "    7: 60,\n",
    "    8: 80, \n",
    "}\n",
    "\n",
    "counter = 1\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height(), '.2f'), \n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                   ha = 'center', va = 'center', \n",
    "                   size=numbers_size,\n",
    "                   xytext = (0, distances_final[counter]), \n",
    "                   textcoords = 'offset points')\n",
    "    counter+=1\n",
    "\n",
    "#plt.savefig(\"results.pdf\", bbox_inches='tight')\n",
    "#plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
